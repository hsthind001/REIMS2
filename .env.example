# API Keys (Required to enable respective provider)
ANTHROPIC_API_KEY="your_anthropic_api_key_here"       # Required: Format: sk-ant-api03-...
PERPLEXITY_API_KEY="your_perplexity_api_key_here"     # Optional: Format: pplx-...
OPENAI_API_KEY="your_openai_api_key_here"             # Optional, for OpenAI models. Format: sk-proj-...
GOOGLE_API_KEY="your_google_api_key_here"             # Optional, for Google Gemini models.
MISTRAL_API_KEY="your_mistral_key_here"               # Optional, for Mistral AI models.
XAI_API_KEY="YOUR_XAI_KEY_HERE"                       # Optional, for xAI AI models.
GROQ_API_KEY="YOUR_GROQ_KEY_HERE"                     # Optional, for Groq models.
OPENROUTER_API_KEY="YOUR_OPENROUTER_KEY_HERE"         # Optional, for OpenRouter models.
AZURE_OPENAI_API_KEY="your_azure_key_here"            # Optional, for Azure OpenAI models (requires endpoint in .taskmaster/config.json).
OLLAMA_API_KEY="your_ollama_api_key_here"             # Optional: For remote Ollama servers that require authentication.
GITHUB_API_KEY="your_github_api_key_here"             # Optional: For GitHub import/export features. Format: ghp_... or github_pat_...

# ============================================================================
# OPEN-SOURCE LLM CONFIGURATION (Market Intelligence AI)
# ============================================================================
# Primary LLM Provider
LLM_PROVIDER=ollama  # Options: "ollama" (local), "groq" (cloud)

# Ollama Configuration (Local Inference)
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_DEFAULT_MODEL=llama3.3:70b-instruct-q4_K_M
OLLAMA_GPU_LAYERS=-1  # -1 = all layers on GPU, 0 = CPU only
OLLAMA_NUM_THREADS=8  # CPU threads for inference

# Groq Configuration (Cloud Fallback - get free key at console.groq.com)
GROQ_API_KEY=  # Optional: Ultra-fast cloud inference (800 tok/s)

# LLM Generation Parameters
LLM_TEMPERATURE=0.3  # Lower = more consistent, higher = more creative
LLM_MAX_TOKENS=4000
LLM_ENABLE_STREAMING=true  # Stream responses for real-time UI
LLM_ENABLE_CACHING=true  # Cache LLM responses (semantic caching)
LLM_CACHE_TTL_HOURS=24  # Cache duration
LLM_AUTO_SELECT=true  # Automatically choose model based on task

# AI Insights (Market Intelligence) tuning
AI_INSIGHTS_LLM_TIMEOUT_SEC=90  # Timeout per LLM section (swot/recommendation/risk/etc.)
AI_INSIGHTS_MAX_TOKENS=2000  # Cap tokens for AI Insights sections
AI_INSIGHTS_PREFER_FAST=false  # Prefer smaller installed models for faster responses
