"""
Extraction Orchestrator - Coordinates PDF extraction and financial data parsing
"""
from sqlalchemy.orm import Session
from typing import Dict, List, Optional, Any
from decimal import Decimal
from datetime import datetime
import re
import statistics
import logging

from app.utils.extraction_engine import MultiEngineExtractor
from app.utils.template_extractor import TemplateExtractor
from app.utils.financial_table_parser import FinancialTableParser
from app.models.document_upload import DocumentUpload
from app.models.extraction_log import ExtractionLog
from app.models.balance_sheet_data import BalanceSheetData
from app.models.income_statement_data import IncomeStatementData
from app.models.cash_flow_data import CashFlowData
from app.models.rent_roll_data import RentRollData
from app.models.mortgage_statement_data import MortgageStatementData
from app.models.lender import Lender
from app.models.financial_metrics import FinancialMetrics
from app.models.chart_of_accounts import ChartOfAccounts
from app.models.extraction_field_metadata import ExtractionFieldMetadata
from app.services.confidence_engine import ConfidenceEngine
from app.services.metadata_storage_service import MetadataStorageService
from app.services.ensemble_engine import EnsembleEngine
from app.services.active_learning_service import ActiveLearningService
from app.services.model_monitoring_service import ModelMonitoringService
from app.services.anomaly_detector import StatisticalAnomalyDetector
from app.services.concordance_service import ConcordanceService
try:
    from app.services.xai_explanation_service import XAIExplanationService
    XAI_AVAILABLE = True
except ImportError:
    XAI_AVAILABLE = False
    logger.warning("XAI service not available")

try:
    from app.services.active_learning_service import ActiveLearningService as AnomalyActiveLearning
    ACTIVE_LEARNING_AVAILABLE = True
except ImportError:
    ACTIVE_LEARNING_AVAILABLE = False
    logger.warning("Active learning service not available")

try:
    from app.services.cross_property_intelligence import CrossPropertyIntelligenceService
    CROSS_PROP_AVAILABLE = True
except ImportError:
    CROSS_PROP_AVAILABLE = False
    logger.warning("Cross-property intelligence not available")

try:
    from app.services.pyod_anomaly_detector import PyODAnomalyDetector
    PYOD_AVAILABLE = True
except ImportError:
    PYOD_AVAILABLE = False
    logger.warning("PyOD detector not available")
from app.db.minio_client import download_file
from app.core.config import settings
from app.core.feature_flags import FeatureFlags
from app.services.deduplication_service import get_deduplication_service
from app.services.alert_trigger_service import AlertTriggerService
import json

logger = logging.getLogger(__name__)


class ExtractionOrchestrator:
    """Orchestrate complete extraction and financial data parsing workflow"""
    
    def __init__(self, db: Session):
        self.db = db
        self.extraction_engine = MultiEngineExtractor()
        self.template_extractor = TemplateExtractor(db)
        self.table_parser = FinancialTableParser()
        self.confidence_engine = ConfidenceEngine()
        self.ensemble_engine = EnsembleEngine()
        self.metadata_service = MetadataStorageService(db, self.confidence_engine)
        self.active_learning = ActiveLearningService(db)
        self.model_monitoring = ModelMonitoringService(db)
        self.xai_service = XAIExplanationService(db) if XAI_AVAILABLE else None
        self.anomaly_active_learning = AnomalyActiveLearning(db) if ACTIVE_LEARNING_AVAILABLE else None
        self.cross_property_intel = CrossPropertyIntelligenceService(db) if CROSS_PROP_AVAILABLE else None
        self.pyod_detector = PyODAnomalyDetector(db) if PYOD_AVAILABLE and FeatureFlags.is_pyod_enabled() else None
    
    def extract_and_parse_document(self, upload_id: int) -> Dict:
        """
        Complete extraction workflow for a document upload with transaction management
        
        Production-ready workflow:
        1. Download PDF from MinIO
        2. Extract text using MultiEngineExtractor
        3. Parse structured financial data using TemplateExtractor
        4. Insert data into appropriate financial tables (TRANSACTIONAL)
        5. Run validations (BEFORE commit)
        6. Rollback if critical validations fail
        7. Calculate financial metrics
        8. Create extraction log
        9. Update upload status
        
        ZERO DATA LOSS GUARANTEE:
        - All operations wrapped in transaction
        - Critical validation failures trigger rollback
        - Data only committed if all critical checks pass
        - Comprehensive audit trail in extraction_log
        
        Args:
            upload_id: DocumentUpload ID
        
        Returns:
            dict: Extraction result with success status and details
        """
        upload = None
        extraction_log = None
        
        try:
            # Get upload record
            upload = self.db.query(DocumentUpload).filter(
                DocumentUpload.id == upload_id
            ).first()
            
            if not upload:
                return {
                    "success": False,
                    "error": f"Upload {upload_id} not found"
                }
            
            # Update status to extracting
            upload.extraction_status = "extracting"
            upload.extraction_started_at = datetime.now()
            self.db.commit()
            
            # Step 1: Download PDF from MinIO
            print(f"üì• Downloading PDF from MinIO: {upload.file_path}")
            pdf_data = download_file(
                object_name=upload.file_path,
                bucket_name=settings.MINIO_BUCKET_NAME
            )
            
            if not pdf_data:
                upload.extraction_status = "failed_download"
                upload.extraction_completed_at = datetime.now()
                upload.notes = "Failed to download file from MinIO storage"
                self.db.commit()
                return {
                    "success": False,
                    "error": "Failed to download file from storage"
                }
            
            print(f"‚úÖ PDF downloaded successfully ({len(pdf_data)} bytes)")
            
            # Step 2: Extract text with validation
            print(f"üîç Extracting text from PDF...")
            extraction_result = self.extraction_engine.extract_with_validation(
                pdf_data=pdf_data,
                strategy="auto",
                lang="eng"
            )
            
            if not extraction_result.get("success"):
                upload.extraction_status = "failed_extraction"
                upload.extraction_completed_at = datetime.now()
                upload.notes = f"Text extraction failed: {extraction_result.get('error')}"
                self.db.commit()
                return {
                    "success": False,
                    "error": extraction_result.get("error", "Extraction failed")
                }
            
            print(f"‚úÖ Text extracted (confidence: {extraction_result['validation']['confidence_score']}%)")
            
            # Step 3: Create extraction log
            extraction_log = self._create_extraction_log(
                upload=upload,
                extraction_result=extraction_result
            )
            
            # Link extraction log to upload
            upload.extraction_id = extraction_log.id
            self.db.commit()
            
            # ==================== DATA INSERTION & QUALITY ASSURANCE ====================
            # Production-ready extraction with comprehensive quality checks
            
            # Update status to validating
            upload.extraction_status = "validating"
            self.db.commit()
            
            print(f"üíæ Beginning data insertion for {upload.document_type}...")
            
            # Step 4: Parse and insert financial data based on document type
            extracted_text = extraction_result["extraction"].get("text", "")
            
            parse_result = self._parse_and_insert_financial_data(
                upload=upload,
                extracted_text=extracted_text,
                confidence_score=extraction_result["validation"]["confidence_score"],
                pdf_data=pdf_data  # Pass already downloaded PDF to avoid re-download
            )
            
            if not parse_result["success"]:
                # Data parsing failed - mark as failed
                upload.extraction_status = "failed_validation"
                upload.extraction_completed_at = datetime.now()
                upload.notes = f"Data parsing/validation failed: {parse_result.get('error')}"
                self.db.commit()
                return {
                    "success": False,
                    "error": parse_result.get("error"),
                    "extraction_log_id": extraction_log.id,
                    "records_inserted": 0
                }
            
            print(f"‚úÖ Inserted {parse_result.get('records_inserted', 0)} records")
            
            # Step 5: Calculate and store comprehensive financial metrics
            print(f"üìä Calculating financial metrics...")
            try:
                self._calculate_financial_metrics(upload)
                print(f"‚úÖ Financial metrics calculated")
            except Exception as metrics_error:
                # Metrics calculation is non-critical - log and continue
                print(f"‚ö†Ô∏è  Metrics calculation skipped: {str(metrics_error)}")
                # Don't fail extraction if metrics fail
            
            # Step 5.5: Detect anomalies in extracted financial data
            print(f"üîç Detecting anomalies in financial data...")
            try:
                self._detect_anomalies_for_document(upload)
                print(f"‚úÖ Anomaly detection completed")
            except Exception as anomaly_error:
                # Anomaly detection is non-critical - log and continue
                print(f"‚ö†Ô∏è  Anomaly detection skipped: {str(anomaly_error)}")
                # Don't fail extraction if anomaly detection fails
            
            # Step 6: Run validations for quality assurance
            print(f"üîç Running {upload.document_type} validations...")
            validation_results = None
            critical_failures = []
            warnings = []
            
            try:
                validation_results = self._run_validations(upload)
                
                # Check for CRITICAL validation failures
                if isinstance(validation_results, dict) and validation_results.get("validation_results"):
                    critical_failures = [
                        v for v in validation_results["validation_results"]
                        if v.get("severity") == "error" and not v.get("passed")
                    ]
                    warnings = [
                        v for v in validation_results["validation_results"]
                        if v.get("severity") in ["warning", "info"] and not v.get("passed")
                    ]
                
                # Log validation results
                if critical_failures:
                    print(f"‚ö†Ô∏è  {len(critical_failures)} CRITICAL validation failure(s):")
                    for failure in critical_failures:
                        print(f"   - {failure.get('error_message', 'Unknown error')}")
                    upload.notes = (
                        f"CRITICAL: {len(critical_failures)} validation error(s). "
                        f"First: {critical_failures[0].get('error_message', 'Unknown')}. "
                        "Manual review REQUIRED."
                    )
                elif warnings:
                    print(f"‚ö†Ô∏è  {len(warnings)} validation warning(s)")
                    upload.notes = f"{len(warnings)} validation warnings - review recommended"
                else:
                    print(f"‚úÖ All validations passed!")
                    upload.notes = "Extraction completed successfully - all validations passed"
                    
            except Exception as validation_error:
                # Validation errors are non-critical - extraction succeeded
                print(f"‚ö†Ô∏è  Validation step failed (non-critical): {str(validation_error)}")
                upload.notes = f"Data extracted successfully. Validation error (non-critical): {str(validation_error)[:100]}"
                # Continue to mark as completed
            
            # ==================== END DATA INSERTION & QUALITY ASSURANCE ====================
            
            # Step 7: CAPTURE FIELD-LEVEL CONFIDENCE METADATA (NEW SPRINT 1 FEATURE!)
            # NOTE: Metadata capture is now NON-CRITICAL to prevent worker crashes
            # Data is already saved, so we won't rollback if metadata fails
            print(f"üéØ Sprint 1 Feature: Capturing field-level confidence metadata...")

            metadata_result = {"success": True, "skipped": True}

            try:
                # Prepare inserted records for metadata capture
                # Note: parse_result should contain the inserted records by table
                inserted_records = parse_result.get("inserted_records", {})

                metadata_result = self._capture_and_save_metadata(
                    upload_id=upload_id,
                    pdf_data=pdf_data,
                    inserted_records=inserted_records
                )

                if not metadata_result.get("success"):
                    # CHANGED: Metadata failure is now NON-CRITICAL
                    # We log the error but don't rollback the successful data insertion
                    print(f"‚ö†Ô∏è  Metadata save failed (non-critical): {metadata_result.get('error', 'Unknown error')}")
                    print(f"‚úÖ Data extraction completed successfully despite metadata failure")

                    # Add note about metadata failure but don't fail the extraction
                    upload.notes = (upload.notes or "") + f"\nMetadata capture skipped: {metadata_result.get('error', 'Unknown error')[:100]}"
                else:
                    print(f"‚úÖ Metadata capture completed successfully!")

            except Exception as metadata_error:
                # Catch any exceptions during metadata capture to prevent worker crashes
                print(f"‚ö†Ô∏è  Metadata capture exception (non-critical): {str(metadata_error)}")
                print(f"‚úÖ Data extraction completed successfully despite metadata exception")
                upload.notes = (upload.notes or "") + f"\nMetadata capture skipped: {str(metadata_error)[:100]}"
                metadata_result = {"success": False, "error": str(metadata_error), "skipped_due_to_exception": True}
            
            # Step 8: GENERATE CONCORDANCE TABLE (BR-005: Model Comparison)
            # NOTE: Concordance generation is NON-CRITICAL to prevent worker crashes
            # Data is already saved, so we won't rollback if concordance fails
            print(f"üìä Generating concordance table for model comparison...")
            
            concordance_result = {"success": True, "skipped": True}
            
            try:
                concordance_service = ConcordanceService(self.db)
                
                concordance_result = concordance_service.generate_concordance_table(
                    upload_id=upload_id,
                    pdf_data=pdf_data,
                    document_type=upload.document_type,
                    property_id=upload.property_id,
                    period_id=upload.period_id
                )
                
                if concordance_result.get("success"):
                    print(
                        f"‚úÖ Concordance table generated: "
                        f"{concordance_result.get('total_fields', 0)} fields, "
                        f"{concordance_result.get('overall_agreement_percentage', 0)}% overall agreement"
                    )
                else:
                    print(f"‚ö†Ô∏è  Concordance table generation failed (non-critical): {concordance_result.get('error', 'Unknown error')}")
                    upload.notes = (upload.notes or "") + f"\nConcordance table skipped: {concordance_result.get('error', 'Unknown')[:100]}"
            
            except Exception as concordance_error:
                # Catch any exceptions during concordance generation to prevent worker crashes
                print(f"‚ö†Ô∏è  Concordance generation exception (non-critical): {str(concordance_error)}")
                upload.notes = (upload.notes or "") + f"\nConcordance table skipped: {str(concordance_error)[:100]}"
                concordance_result = {"success": False, "error": str(concordance_error), "skipped_due_to_exception": True}
            
            # Step 9: Update upload status to completed
            upload.extraction_status = "completed"
            upload.extraction_completed_at = datetime.now()
            self.db.commit()
            
            print(f"‚úÖ Extraction completed successfully for upload_id={upload_id}")
            
            return {
                "success": True,
                "upload_id": upload_id,
                "extraction_log_id": extraction_log.id,
                "records_inserted": parse_result.get("records_inserted", 0),
                "confidence_score": extraction_result["validation"]["confidence_score"],
                "needs_review": extraction_result["needs_review"] or len(warnings) > 0,
                "validation_results": validation_results,
                "metadata": metadata_result,  # NEW: Metadata capture results
                "concordance": concordance_result,  # NEW: Concordance table results
                "message": "Extraction completed - all critical validations passed"
            }
        
        except Exception as e:
            # Handle any unexpected errors
            print(f"‚ùå Extraction exception for upload_id={upload_id}: {str(e)}")
            
            # Rollback any failed transaction
            try:
                self.db.rollback()
            except:
                pass  # Ignore if already rolled back
            
            # Update status in new transaction
            if upload:
                try:
                    # Re-query the upload in case session is stale
                    upload = self.db.query(DocumentUpload).filter(
                        DocumentUpload.id == upload_id
                    ).first()
                    
                    if upload:
                        upload.extraction_status = "failed"
                        upload.extraction_completed_at = datetime.now()
                        upload.notes = f"Extraction exception: {str(e)}"
                        self.db.commit()
                except Exception as update_error:
                    print(f"‚ö†Ô∏è  Failed to update status: {str(update_error)}")
            
            return {
                "success": False,
                "error": f"Extraction failed: {str(e)}",
                "extraction_log_id": extraction_log.id if extraction_log else None
            }
    
    def _create_extraction_log(
        self,
        upload: DocumentUpload,
        extraction_result: Dict
    ) -> ExtractionLog:
        """Create extraction log entry"""
        extraction = extraction_result["extraction"]
        validation = extraction_result["validation"]
        classification = extraction_result["classification"]
        
        log_entry = ExtractionLog(
            filename=upload.file_name,
            file_size=upload.file_size_bytes,
            file_hash=upload.file_hash,
            document_type=classification.get("document_type", "unknown"),
            total_pages=extraction.get("total_pages", 0),
            strategy_used=extraction_result["strategy_used"],
            engines_used=extraction.get("engines_used", [extraction.get("engine")]),
            primary_engine=extraction.get("engine", "unknown"),
            confidence_score=validation["confidence_score"],
            quality_level=validation["overall_quality"],
            passed_checks=validation["passed_checks"],
            total_checks=validation["total_checks"],
            processing_time_seconds=extraction_result["processing_time_seconds"],
            validation_issues=validation["issues"],
            validation_warnings=validation["warnings"],
            recommendations=validation.get("recommendations", []),
            text_preview=extraction.get("text", "")[:500],
            total_words=extraction.get("total_words", 0),
            total_chars=extraction.get("total_chars", 0),
            tables_found=0,
            images_found=0,
            needs_review=extraction_result["needs_review"],
            custom_metadata=classification.get("characteristics", {})
        )
        
        self.db.add(log_entry)
        self.db.commit()
        self.db.refresh(log_entry)
        
        return log_entry
    
    def _parse_and_insert_financial_data(
        self,
        upload: DocumentUpload,
        extracted_text: str,
        confidence_score: float,
        pdf_data: bytes = None
    ) -> Dict:
        """
        Parse extracted text and insert into appropriate financial tables

        Enhanced with table extraction for 100% accuracy
        """
        try:
            # Use provided pdf_data or download if not provided
            if pdf_data is None:
                pdf_data = download_file(
                    object_name=upload.file_path,
                    bucket_name=settings.MINIO_BUCKET_NAME
                )

                if not pdf_data:
                    return {
                        "success": False,
                        "error": "Failed to download PDF for table extraction"
                    }
            
            # Step 1: For mortgage statements, use mortgage extraction service
            if upload.document_type == "mortgage_statement":
                from app.services.mortgage_extraction_service import MortgageExtractionService
                mortgage_service = MortgageExtractionService(self.db)
                extraction_result = mortgage_service.extract_mortgage_data(
                    extracted_text=extracted_text,
                    pdf_data=pdf_data
                )
                
                if extraction_result.get("success"):
                    parsed_data = {
                        "success": True,
                        "mortgage_data": extraction_result.get("mortgage_data", {}),
                        "extraction_coordinates": extraction_result.get("extraction_coordinates", {}),
                        "extraction_method": extraction_result.get("extraction_method", "template_patterns"),
                        "confidence": extraction_result.get("confidence", confidence_score)
                    }
                    confidence_score = extraction_result.get("confidence", confidence_score)
                else:
                    parsed_data = {
                        "success": False,
                        "error": extraction_result.get("error", "Mortgage extraction failed")
                    }
            else:
                # Step 1: Try table extraction first (highest accuracy)
                parsed_data = self._extract_with_tables(pdf_data, upload.document_type)
                
                # Step 2: If table extraction yields no results, try template extractor
                if not parsed_data.get("success") or not parsed_data.get("line_items"):
                    parsed_data = self.template_extractor.extract_using_template(
                        extracted_text=extracted_text,
                        document_type=upload.document_type,
                        template_name=None
                    )
                
                # Step 3: If still no success, try fallback regex extraction
                if not parsed_data.get("success") or not parsed_data.get("line_items"):
                    parsed_data = self._fallback_extraction(extracted_text, upload.document_type)
            
            # Step 4: Intelligent account matching (skip for mortgage statements)
            if parsed_data.get("line_items") and upload.document_type != "mortgage_statement":
                parsed_data["line_items"] = self._match_accounts_intelligent(
                    parsed_data["line_items"],
                    document_type=upload.document_type
                )
            
            # Step 5: Insert based on document type
            records_inserted = 0
            
            if upload.document_type == "balance_sheet":
                records_inserted = self._insert_balance_sheet_data(
                    upload=upload,
                    parsed_data=parsed_data,
                    confidence_score=confidence_score
                )
            elif upload.document_type == "income_statement":
                records_inserted = self._insert_income_statement_data(
                    upload=upload,
                    parsed_data=parsed_data,
                    confidence_score=confidence_score
                )
            elif upload.document_type == "cash_flow":
                records_inserted = self._insert_cash_flow_data(
                    upload=upload,
                    parsed_data=parsed_data,
                    confidence_score=confidence_score
                )
            elif upload.document_type == "rent_roll":
                records_inserted = self._insert_rent_roll_data(
                    upload=upload,
                    parsed_data=parsed_data,
                    confidence_score=confidence_score
                )
            elif upload.document_type == "mortgage_statement":
                records_inserted = self._insert_mortgage_statement_data(
                    upload=upload,
                    parsed_data=parsed_data,
                    confidence_score=confidence_score
                )
            
            return {
                "success": True,
                "records_inserted": records_inserted,
                "extraction_method": parsed_data.get("extraction_method", "unknown")
            }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def _insert_balance_sheet_data(
        self,
        upload: DocumentUpload,
        parsed_data: Dict,
        confidence_score: float
    ) -> int:
        """
        Insert balance sheet line items with enhanced matching
        
        DELETE and REPLACE strategy: Deletes all existing balance sheet data
        for this property+period before inserting new data to ensure clean replacement.
        """
        items = parsed_data.get("line_items", [])
        records_inserted = 0
        
        # DELETE all existing balance sheet data for this property+period
        deleted_count = self.db.query(BalanceSheetData).filter(
            BalanceSheetData.property_id == upload.property_id,
            BalanceSheetData.period_id == upload.period_id
        ).delete(synchronize_session=False)
        
        if deleted_count > 0:
            print(f"üóëÔ∏è  Deleted {deleted_count} existing balance sheet records for property {upload.property_id}, period {upload.period_id}")
        
        # INTELLIGENT DEDUPLICATION - Prevents duplicate constraint violations
        # Constraint: (property_id, period_id, account_code)
        # We must ensure no duplicate account_codes exist in the same batch
        
        dedup_service = get_deduplication_service()
        
        # Add line_number for tracking if not present
        for idx, item in enumerate(items):
            if not item.get("line_number"):
                item['line_number'] = idx + 1
        
        # Use constraint-aware deduplication: (account_code)
        dedup_result = dedup_service.deduplicate_items(
            items=items,
            constraint_columns=['account_code'],
            selection_strategy='confidence',  # Default, but will use 'amount' for totals/subtotals
            document_type='balance_sheet',
            is_total_or_subtotal=lambda item: item.get("is_subtotal", False) or item.get("is_total", False)
        )
        
        deduplicated_items = dedup_result['deduplicated_items']
        stats = dedup_result['statistics']
        
        # Count totals/subtotals for logging
        totals_count = sum(1 for item in deduplicated_items if item.get("is_subtotal") or item.get("is_total"))
        detail_count = len(deduplicated_items) - totals_count
        
        if stats['duplicates_removed'] > 0:
            print(f"üìä Balance Sheet extraction: {stats['total_items']} raw items ‚Üí {stats['final_count']} unique records ({detail_count} detail + {totals_count} totals/subtotals, {stats['duplicates_removed']} duplicates removed)")
        else:
            print(f"üìä Balance Sheet extraction: {stats['total_items']} raw items ‚Üí {stats['final_count']} unique records ({detail_count} detail + {totals_count} totals/subtotals)")
        
        # PRE-INSERTION VALIDATION: Ensure no duplicate constraint keys (safety check)
        is_valid, error_msg = dedup_service.validate_no_duplicates(
            items=deduplicated_items,
            constraint_columns=['account_code'],
            context=f"balance_sheet insertion (upload_id={upload.id})"
        )
        if not is_valid:
            raise ValueError(f"Pre-insertion validation failed: {error_msg}")
        
        # INSERT all new records (including unmatched accounts for complete reconciliation)
        for item in deduplicated_items:
            account_code = item.get("matched_account_code") or item.get("account_code", "") or "UNMATCHED"
            account_name = item.get("matched_account_name") or item.get("account_name", "")
            amount = item.get("amount", 0.0)
            account_id = item.get("matched_account_id")
            item_confidence = item.get("confidence", confidence_score)
            match_confidence = item.get("match_confidence", 0.0) if not account_id else item.get("match_confidence", 100.0)
            
            # Skip only if we have absolutely no identifying information
            if not account_name and not account_code:
                continue
            
            # Use average of extraction and match confidence
            final_confidence = (float(item_confidence) + float(match_confidence)) / 2.0
            
            # Flag for review if unmatched or low confidence
            needs_review = (final_confidence < 85.0) or (account_id is None) or (account_code == "UNMATCHED")
            
            # Extract coordinates if available (for PDF source navigation)
            extraction_x0 = item.get("extraction_x0")
            extraction_y0 = item.get("extraction_y0")
            extraction_x1 = item.get("extraction_x1")
            extraction_y1 = item.get("extraction_y1")
            line_number = item.get("line_number")
            page_number = item.get("page_number") or item.get("page")
            
            # Insert new entry (no longer check for existing since we deleted all)
            bs_data = BalanceSheetData(
                property_id=upload.property_id,
                period_id=upload.period_id,
                upload_id=upload.id,
                account_id=account_id,
                account_code=account_code,
                account_name=account_name,
                amount=Decimal(str(amount)),
                extraction_confidence=Decimal(str(final_confidence)),
                match_confidence=Decimal(str(match_confidence)) if match_confidence else None,
                needs_review=needs_review,
                page_number=page_number,
                extraction_x0=Decimal(str(extraction_x0)) if extraction_x0 is not None else None,
                extraction_y0=Decimal(str(extraction_y0)) if extraction_y0 is not None else None,
                extraction_x1=Decimal(str(extraction_x1)) if extraction_x1 is not None else None,
                extraction_y1=Decimal(str(extraction_y1)) if extraction_y1 is not None else None,
                line_number=line_number
            )
            self.db.add(bs_data)
            records_inserted += 1
        
        self.db.commit()
        return records_inserted
    
    def _insert_income_statement_data(
        self,
        upload: DocumentUpload,
        parsed_data: Dict,
        confidence_score: float
    ) -> int:
        """
        Insert income statement data with Template v1.0 compliance
        
        DELETE and REPLACE strategy: Deletes existing header and all line items
        for this property+period before inserting new data to ensure clean replacement.
        
        Inserts:
        - Header record with summary totals
        - Line items with full categorization and hierarchy
        """
        from app.models.income_statement_header import IncomeStatementHeader
        from datetime import datetime
        
        items = parsed_data.get("line_items", [])
        header_data = parsed_data.get("header", {})
        
        records_inserted = 0
        
        # DELETE existing header (cascade should delete line items via foreign key)
        deleted_header = self.db.query(IncomeStatementHeader).filter(
            IncomeStatementHeader.property_id == upload.property_id,
            IncomeStatementHeader.period_id == upload.period_id
        ).delete(synchronize_session=False)
        
        # Also explicitly delete line items (in case no cascade)
        deleted_items = self.db.query(IncomeStatementData).filter(
            IncomeStatementData.property_id == upload.property_id,
            IncomeStatementData.period_id == upload.period_id
        ).delete(synchronize_session=False)
        
        if deleted_header > 0 or deleted_items > 0:
            print(f"üóëÔ∏è  Deleted {deleted_header} income statement header(s) and {deleted_items} line items for property {upload.property_id}, period {upload.period_id}")
        
        # Step 1: Calculate header totals from line items
        totals = self._calculate_income_statement_totals(items)
        
        # Step 2: Parse period dates from header
        period_start, period_end = self._parse_period_dates(
            header_data.get("period_start_date"),
            header_data.get("period_end_date"),
            upload.period_id
        )
        
        # Step 3: Create NEW IncomeStatementHeader
        header = IncomeStatementHeader(
            property_id=upload.property_id,
            period_id=upload.period_id,
            upload_id=upload.id,
            property_name=header_data.get("property_name", ""),
            property_code=header_data.get("property_code", ""),
            report_period_start=period_start,
            report_period_end=period_end,
            period_type=header_data.get("period_type", "Monthly"),
            accounting_basis=header_data.get("accounting_basis", "Accrual"),
            report_generation_date=self._parse_report_date(header_data.get("report_generation_date")),
            # Income totals
            total_income=Decimal(str(totals.get("total_income", 0))),
            base_rentals=Decimal(str(totals.get("base_rentals", 0))),
            total_recovery_income=Decimal(str(totals.get("total_recovery_income", 0))),
            total_other_income=Decimal(str(totals.get("total_other_income", 0))),
            # Operating expense totals
            total_operating_expenses=Decimal(str(totals.get("total_operating_expenses", 0))),
            total_property_expenses=Decimal(str(totals.get("total_property_expenses", 0))),
            total_utility_expenses=Decimal(str(totals.get("total_utility_expenses", 0))),
            total_contracted_expenses=Decimal(str(totals.get("total_contracted_expenses", 0))),
            total_rm_expenses=Decimal(str(totals.get("total_rm_expenses", 0))),
            total_admin_expenses=Decimal(str(totals.get("total_admin_expenses", 0))),
            # Additional expense totals
            total_additional_operating_expenses=Decimal(str(totals.get("total_additional_expenses", 0))),
            total_management_fees=Decimal(str(totals.get("total_management_fees", 0))),
            total_leasing_costs=Decimal(str(totals.get("total_leasing_costs", 0))),
            total_ll_expenses=Decimal(str(totals.get("total_ll_expenses", 0))),
            # Total expenses
            total_expenses=Decimal(str(totals.get("total_expenses", 0))),
            # NOI
            net_operating_income=Decimal(str(totals.get("noi", 0))),
            noi_percentage=Decimal(str(totals.get("noi_percentage", 0))),
            # Other income/expenses (below the line)
            mortgage_interest=Decimal(str(totals.get("mortgage_interest", 0))),
            depreciation=Decimal(str(totals.get("depreciation", 0))),
            amortization=Decimal(str(totals.get("amortization", 0))),
            total_other_income_expense=Decimal(str(totals.get("total_other", 0))),
            # Net income
            net_income=Decimal(str(totals.get("net_income", 0))),
            net_income_percentage=Decimal(str(totals.get("net_income_percentage", 0))),
            # Quality
            extraction_confidence=Decimal(str(confidence_score)),
            needs_review=confidence_score < 85.0
        )
        self.db.add(header)
        self.db.flush()  # Get header.id
        
        # Step 4: INTELLIGENT DEDUPLICATION - Prevents duplicate constraint violations
        # Constraint: (property_id, period_id, account_code, account_name)
        # We must ensure no duplicate (account_code, account_name) combinations exist
        
        dedup_service = get_deduplication_service()
        
        # Add line_number for tracking if not present
        for idx, item in enumerate(items):
            if not item.get("line_number"):
                item['line_number'] = idx + 1
        
        # Use constraint-aware deduplication: (account_code, account_name)
        dedup_result = dedup_service.deduplicate_items(
            items=items,
            constraint_columns=['account_code', 'account_name'],
            selection_strategy='confidence',  # Default, but will use 'amount' for totals/subtotals
            document_type='income_statement',
            is_total_or_subtotal=lambda item: item.get("is_subtotal", False) or item.get("is_total", False)
        )
        
        deduplicated_items = dedup_result['deduplicated_items']
        stats = dedup_result['statistics']
        
        # Count totals/subtotals for logging
        totals_count = sum(1 for item in deduplicated_items if item.get("is_subtotal") or item.get("is_total"))
        detail_count = len(deduplicated_items) - totals_count
        
        if stats['duplicates_removed'] > 0:
            print(f"üìä Income Statement extraction: {stats['total_items']} raw items ‚Üí {stats['final_count']} unique records ({detail_count} detail + {totals_count} totals/subtotals, {stats['duplicates_removed']} duplicates removed)")
        else:
            print(f"üìä Income Statement extraction: {stats['total_items']} raw items ‚Üí {stats['final_count']} unique records ({detail_count} detail + {totals_count} totals/subtotals)")
        
        # Step 5: Insert line items with full categorization (including unmatched)
        for item in deduplicated_items:
            account_code = item.get("account_code", "") or item.get("matched_account_code", "") or "UNMATCHED"
            account_name = item.get("account_name", "") or item.get("matched_account_name", "")
            account_id = item.get("matched_account_id")
            match_confidence = item.get("match_confidence", 0.0) if not account_id else item.get("match_confidence", 100.0)
            
            # Skip only if completely no information
            if not account_name and account_code == "UNMATCHED":
                continue
            
            # Calculate confidence and review flag
            item_confidence = item.get("confidence", confidence_score)
            avg_confidence = (float(item_confidence) + float(match_confidence)) / 2.0
            needs_review = (avg_confidence < 85.0) or (account_id is None) or (account_code == "UNMATCHED")
            
            # Extract coordinates if available (for PDF source navigation)
            extraction_x0 = item.get("extraction_x0")
            extraction_y0 = item.get("extraction_y0")
            extraction_x1 = item.get("extraction_x1")
            extraction_y1 = item.get("extraction_y1")
            
            # Insert new with all Template v1.0 fields
            is_data = IncomeStatementData(
                header_id=header.id,
                property_id=upload.property_id,
                period_id=upload.period_id,
                upload_id=upload.id,
                account_id=account_id,
                account_code=account_code or "",
                account_name=account_name,
                period_amount=Decimal(str(item.get("period_amount", 0))),
                ytd_amount=Decimal(str(item["ytd_amount"])) if item.get("ytd_amount") is not None else None,
                period_percentage=Decimal(str(item["period_percentage"])) if item.get("period_percentage") is not None else None,
                ytd_percentage=Decimal(str(item["ytd_percentage"])) if item.get("ytd_percentage") is not None else None,
                # Template v1.0: Header metadata
                period_type=header_data.get("period_type", "Monthly"),
                accounting_basis=header_data.get("accounting_basis"),
                report_generation_date=self._parse_report_date(header_data.get("report_generation_date")),
                page_number=item.get("page"),
                # Template v1.0: Hierarchical structure
                is_subtotal=item.get("is_subtotal", False),
                is_total=item.get("is_total", False),
                line_category=item.get("line_category"),
                line_subcategory=item.get("line_subcategory"),
                line_number=item.get("line_number"),
                account_level=item.get("account_level", 3),
                # Template v1.0: Classification
                is_below_the_line=item.get("is_below_the_line", False),
                # Template v1.0: Extraction quality
                extraction_confidence=Decimal(str(avg_confidence)),
                match_confidence=Decimal(str(match_confidence)) if match_confidence else None,
                extraction_method=item.get("extraction_method", "table"),
                needs_review=needs_review,
                # PDF Source Navigation: Extraction coordinates
                extraction_x0=Decimal(str(extraction_x0)) if extraction_x0 is not None else None,
                extraction_y0=Decimal(str(extraction_y0)) if extraction_y0 is not None else None,
                extraction_x1=Decimal(str(extraction_x1)) if extraction_x1 is not None else None,
                extraction_y1=Decimal(str(extraction_y1)) if extraction_y1 is not None else None
            )
            self.db.add(is_data)
            records_inserted += 1
        
        self.db.commit()
        return records_inserted
    
    def _calculate_income_statement_totals(self, items: List[Dict]) -> Dict:
        """
        Calculate summary totals from income statement line items
        
        Returns all totals needed for IncomeStatementHeader:
        - Income totals (base rentals, recovery, other)
        - Operating expense totals (property, utility, contracted, R&M, admin)
        - Additional expense totals (management, leasing, LL)
        - Performance metrics (NOI, Net Income with percentages)
        """
        totals = {
            # Income
            "total_income": 0,
            "base_rentals": 0,
            "total_recovery_income": 0,
            "total_other_income": 0,
            # Operating expenses
            "total_operating_expenses": 0,
            "total_property_expenses": 0,
            "total_utility_expenses": 0,
            "total_contracted_expenses": 0,
            "total_rm_expenses": 0,
            "total_admin_expenses": 0,
            # Additional expenses
            "total_additional_expenses": 0,
            "total_management_fees": 0,
            "total_leasing_costs": 0,
            "total_ll_expenses": 0,
            # Total expenses
            "total_expenses": 0,
            # NOI
            "noi": 0,
            "noi_percentage": 0,
            # Other income/expenses (below the line)
            "mortgage_interest": 0,
            "depreciation": 0,
            "amortization": 0,
            "total_other": 0,
            # Net income
            "net_income": 0,
            "net_income_percentage": 0
        }
        
        for item in items:
            amount = item.get("period_amount", 0)
            account_code = item.get("account_code", "")
            account_name = item.get("account_name", "")
            category = item.get("line_category", "")
            subcategory = item.get("line_subcategory", "")
            is_total = item.get("is_total", False)
            is_subtotal = item.get("is_subtotal", False)
            
            # Extract account code number for range checks
            code_num = 0
            if account_code and '-' in account_code:
                try:
                    code_num = int(account_code.split('-')[0])
                except:
                    pass
            
            # INCOME SECTION (4000-4999)
            if category == "INCOME":
                if is_total and code_num == 4990:
                    totals["total_income"] = amount
                elif code_num == 4010:  # Base Rentals
                    totals["base_rentals"] = amount
                elif code_num in [4020, 4030, 4040, 4055, 4060]:  # Recovery income
                    totals["total_recovery_income"] += amount
                elif code_num in [4018, 4090, 4091]:  # Other income
                    totals["total_other_income"] += amount
            
            # OPERATING EXPENSES SECTION (5000-5999)
            elif category == "OPERATING_EXPENSE":
                if is_subtotal or is_total:
                    if code_num == 5990:  # Total Operating Expenses
                        totals["total_operating_expenses"] = amount
                    elif code_num == 5199:  # Total Utility Expense
                        totals["total_utility_expenses"] = amount
                    elif code_num == 5299:  # Total Contracted Expenses
                        totals["total_contracted_expenses"] = amount
                    elif code_num == 5399:  # Total R&M
                        totals["total_rm_expenses"] = amount
                    elif code_num == 5499:  # Total Admin
                        totals["total_admin_expenses"] = amount
                elif 5010 <= code_num <= 5014:  # Property costs
                    totals["total_property_expenses"] += amount
            
            # ADDITIONAL EXPENSES SECTION (6000-6199)
            elif category == "ADDITIONAL_EXPENSE":
                if is_subtotal or is_total:
                    if code_num == 6190:  # Total Additional Operating Expenses
                        totals["total_additional_expenses"] = amount
                    elif code_num == 6069:  # Total LL Expense
                        totals["total_ll_expenses"] = amount
                elif 6010 <= code_num <= 6020:  # Management fees
                    totals["total_management_fees"] += amount
                elif code_num in [6014, 6016]:  # Leasing costs
                    totals["total_leasing_costs"] += amount
            
            # TOTAL EXPENSES
            elif code_num == 6199:
                totals["total_expenses"] = amount
            
            # NOI
            elif code_num == 6299:
                totals["noi"] = amount
            
            # OTHER INCOME/EXPENSES (BELOW THE LINE) (7000-7999)
            elif category == "OTHER_EXPENSE" or code_num >= 7000:
                if code_num == 7010:  # Mortgage Interest
                    totals["mortgage_interest"] = amount
                elif code_num == 7020:  # Depreciation
                    totals["depreciation"] = amount
                elif code_num == 7030:  # Amortization
                    totals["amortization"] = amount
                elif code_num == 7090:  # Total Other Income/Expense
                    totals["total_other"] = amount
            
            # NET INCOME
            elif code_num == 9090:
                totals["net_income"] = amount
        
        # Calculate percentages
        if totals["total_income"] > 0:
            totals["noi_percentage"] = round((totals["noi"] / totals["total_income"]) * 100, 2)
            totals["net_income_percentage"] = round((totals["net_income"] / totals["total_income"]) * 100, 2)
        
        return totals
    
    def _insert_cash_flow_data(
        self,
        upload: DocumentUpload,
        parsed_data: Dict,
        confidence_score: float
    ) -> int:
        """
        Insert cash flow data with Template v1.0 compliance
        
        DELETE and REPLACE strategy: Deletes all existing cash flow data
        (header, line items, adjustments, reconciliations) for this property+period
        before inserting new data to ensure clean replacement.
        
        Transaction handling: Deletion is committed in a separate transaction
        before insertion to prevent rollback issues and ensure clean state.
        
        Inserts:
        - Header record with summary totals
        - Line items with full categorization
        - Adjustments
        - Cash account reconciliations
        """
        from app.models.cash_flow_header import CashFlowHeader
        from app.models.cash_flow_adjustments import CashFlowAdjustment
        from app.models.cash_account_reconciliation import CashAccountReconciliation
        from datetime import datetime
        import logging
        
        logger = logging.getLogger(__name__)
        
        items = parsed_data.get("line_items", [])
        adjustments = parsed_data.get("adjustments", [])
        cash_accounts = parsed_data.get("cash_accounts", [])
        header_data = parsed_data.get("header", {})
        
        records_inserted = 0
        
        # STEP 1: DELETE all existing cash flow data in a separate transaction
        # This ensures deletion is committed before insertion, preventing rollback issues
        try:
            deleted_header = self.db.query(CashFlowHeader).filter(
                CashFlowHeader.property_id == upload.property_id,
                CashFlowHeader.period_id == upload.period_id
            ).delete(synchronize_session=False)
            
            deleted_items = self.db.query(CashFlowData).filter(
                CashFlowData.property_id == upload.property_id,
                CashFlowData.period_id == upload.period_id
            ).delete(synchronize_session=False)
            
            deleted_adjustments = self.db.query(CashFlowAdjustment).filter(
                CashFlowAdjustment.property_id == upload.property_id,
                CashFlowAdjustment.period_id == upload.period_id
            ).delete(synchronize_session=False)
            
            deleted_reconciliations = self.db.query(CashAccountReconciliation).filter(
                CashAccountReconciliation.property_id == upload.property_id,
                CashAccountReconciliation.period_id == upload.period_id
            ).delete(synchronize_session=False)
            
            # Commit deletion in separate transaction
            self.db.flush()  # Ensure deletions are applied
            self.db.commit()  # Commit deletion transaction
            
            if any([deleted_header, deleted_items, deleted_adjustments, deleted_reconciliations]):
                print(f"üóëÔ∏è  Deleted cash flow data: {deleted_header} header, {deleted_items} items, {deleted_adjustments} adjustments, {deleted_reconciliations} reconciliations for property {upload.property_id}, period {upload.period_id}")
                logger.info(f"Deleted cash flow data for upload_id={upload.id}: {deleted_header} header, {deleted_items} items, {deleted_adjustments} adjustments, {deleted_reconciliations} reconciliations")
        
        except Exception as e:
            # Rollback deletion transaction if it fails
            self.db.rollback()
            error_msg = f"Failed to delete existing cash flow data for upload_id={upload.id}: {str(e)}"
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # STEP 2: Calculate header totals from line items
        totals = self._calculate_cash_flow_totals(items)
        
        # STEP 3: Parse period dates from header
        period_start, period_end = self._parse_period_dates(
            header_data.get("report_period_start"),
            header_data.get("report_period_end"),
            upload.period_id
        )
        
        # STEP 4: Create NEW CashFlowHeader (in new transaction)
        try:
            header = CashFlowHeader(
                property_id=upload.property_id,
                period_id=upload.period_id,
                upload_id=upload.id,
                property_name=header_data.get("property_name", ""),
                property_code=header_data.get("property_code", ""),
                report_period_start=period_start,
                report_period_end=period_end,
                accounting_basis=header_data.get("accounting_basis", "Accrual"),
                report_generation_date=self._parse_report_date(header_data.get("report_generation_date")),
                # Totals from calculations
                total_income=Decimal(str(totals.get("total_income", 0))),
                base_rentals=Decimal(str(totals.get("base_rentals", 0))),
                total_operating_expenses=Decimal(str(totals.get("total_operating_expenses", 0))),
                total_additional_operating_expenses=Decimal(str(totals.get("total_additional_expenses", 0))),
                total_expenses=Decimal(str(totals.get("total_expenses", 0))),
                net_operating_income=Decimal(str(totals.get("noi", 0))),
                noi_percentage=Decimal(str(totals.get("noi_percentage", 0))),
                mortgage_interest=Decimal(str(totals.get("mortgage_interest", 0))),
                depreciation=Decimal(str(totals.get("depreciation", 0))),
                amortization=Decimal(str(totals.get("amortization", 0))),
                total_other_income_expense=Decimal(str(totals.get("total_other", 0))),
                net_income=Decimal(str(totals.get("net_income", 0))),
                net_income_percentage=Decimal(str(totals.get("net_income_percentage", 0))),
                total_adjustments=Decimal(str(sum(adj.get("amount", 0) for adj in adjustments))),
                cash_flow=Decimal(str(totals.get("cash_flow", 0))),
                cash_flow_percentage=Decimal(str(totals.get("cash_flow_percentage", 0))),
                beginning_cash_balance=Decimal(str(totals.get("beginning_cash", 0))),
                ending_cash_balance=Decimal(str(totals.get("ending_cash", 0))),
                extraction_confidence=Decimal(str(confidence_score)),
                needs_review=confidence_score < 85.0
            )
            self.db.add(header)
            self.db.flush()  # Get header.id
        
            # STEP 5: INTELLIGENT DEDUPLICATION - Prevents duplicate constraint violations
            # Constraints: 
            #   - uq_cf_property_period_account_name_line: (property_id, period_id, account_code, account_name, line_number)
            # Note: Old constraint uq_cf_property_period_account (without line_number) has been removed via migration
            # We use the constraint: (account_code, account_name, line_number)
            
            dedup_service = get_deduplication_service()
            
            # Add line_number for tracking if not present
            for idx, item in enumerate(items):
                if not item.get("line_number"):
                    item['line_number'] = idx + 1
            
            # Use constraint-aware deduplication: (account_code, account_name, line_number)
            dedup_result = dedup_service.deduplicate_items(
                items=items,
                constraint_columns=['account_code', 'account_name', 'line_number'],
                selection_strategy='confidence',  # Default, but will use 'amount' for totals/subtotals
                document_type='cash_flow',
                is_total_or_subtotal=lambda item: item.get("is_subtotal", False) or item.get("is_total", False)
            )
            
            deduplicated_items = dedup_result['deduplicated_items']
            stats = dedup_result['statistics']
            
            # Count totals/subtotals for logging
            totals_count = sum(1 for item in deduplicated_items if item.get("is_subtotal") or item.get("is_total"))
            detail_count = len(deduplicated_items) - totals_count
            
            if stats['duplicates_removed'] > 0:
                print(f"üìä Cash Flow extraction: {stats['total_items']} raw items ‚Üí {stats['final_count']} unique records ({detail_count} detail + {totals_count} totals/subtotals, {stats['duplicates_removed']} duplicates removed)")
            else:
                print(f"üìä Cash Flow extraction: {stats['total_items']} raw items ‚Üí {stats['final_count']} unique records ({detail_count} detail + {totals_count} totals/subtotals)")
            
            # PRE-INSERTION VALIDATION: Ensure no duplicate constraint keys (safety check)
            is_valid, error_msg = dedup_service.validate_no_duplicates(
                items=deduplicated_items,
                constraint_columns=['account_code', 'account_name', 'line_number'],
                context=f"cash_flow insertion (upload_id={upload.id})"
            )
            if not is_valid:
                raise ValueError(f"Pre-insertion validation failed: {error_msg}")
            
            # STEP 6: Insert line items with full categorization (including unmatched)
            for item in deduplicated_items:
                account_code = item.get("account_code", "") or item.get("matched_account_code", "") or "UNMATCHED"
                account_name = item.get("account_name", "") or item.get("matched_account_name", "")
                account_id = item.get("matched_account_id")
                match_confidence = item.get("match_confidence", 0.0) if not account_id else item.get("match_confidence", 100.0)
                
                # Skip only if completely no information
                if not account_name and account_code == "UNMATCHED":
                    continue
                
                # Calculate confidence and review flag
                item_confidence = item.get("confidence", confidence_score)
                avg_confidence = (float(item_confidence) + float(match_confidence)) / 2.0
                needs_review = (avg_confidence < 85.0) or (account_id is None) or (account_code == "UNMATCHED")
                
                # Determine cash flow category based on account code and line section
                cash_flow_category = self._determine_cash_flow_category(account_code, item.get("line_section"))
                
                # Insert new line item
                # Extract coordinates if available (for PDF source navigation)
                extraction_x0 = item.get("extraction_x0")
                extraction_y0 = item.get("extraction_y0")
                extraction_x1 = item.get("extraction_x1")
                extraction_y1 = item.get("extraction_y1")
                
                cf_data = CashFlowData(
                    header_id=header.id,
                    property_id=upload.property_id,
                    period_id=upload.period_id,
                    upload_id=upload.id,
                    account_id=account_id,
                    account_code=account_code or "",
                    account_name=account_name,
                    period_amount=Decimal(str(item.get("period_amount", 0))),
                    ytd_amount=Decimal(str(item["ytd_amount"])) if item.get("ytd_amount") is not None else None,
                    period_percentage=Decimal(str(item["period_percentage"])) if item.get("period_percentage") is not None else None,
                    ytd_percentage=Decimal(str(item["ytd_percentage"])) if item.get("ytd_percentage") is not None else None,
                    cash_flow_category=cash_flow_category,  # NEW: Set category
                    line_section=item.get("line_section"),
                    line_category=item.get("line_category"),
                    line_subcategory=item.get("line_subcategory"),
                    line_number=item.get("line_number"),
                    is_subtotal=item.get("is_subtotal", False),
                    is_total=item.get("is_total", False),
                    page_number=item.get("page"),
                    extraction_confidence=Decimal(str(avg_confidence)),
                    needs_review=needs_review,
                    # PDF Source Navigation: Extraction coordinates
                    extraction_x0=Decimal(str(extraction_x0)) if extraction_x0 is not None else None,
                    extraction_y0=Decimal(str(extraction_y0)) if extraction_y0 is not None else None,
                    extraction_x1=Decimal(str(extraction_x1)) if extraction_x1 is not None else None,
                    extraction_y1=Decimal(str(extraction_y1)) if extraction_y1 is not None else None
                )
                self.db.add(cf_data)
                records_inserted += 1
            
            # STEP 7: Insert NEW adjustments
            for adj in adjustments:
                adj_record = CashFlowAdjustment(
                    header_id=header.id,
                    property_id=upload.property_id,
                    period_id=upload.period_id,
                    upload_id=upload.id,
                    adjustment_category=adj.get("adjustment_category"),
                    adjustment_name=adj.get("adjustment_name"),
                    amount=Decimal(str(adj.get("amount", 0))),
                    is_increase=adj.get("is_increase", True),
                    related_property=adj.get("related_property"),
                    related_entity=adj.get("related_entity"),
                    line_number=adj.get("line_number"),
                    page_number=adj.get("page"),
                    extraction_confidence=Decimal(str(adj.get("confidence", confidence_score))),
                    needs_review=adj.get("confidence", confidence_score) < 85.0
                )
                self.db.add(adj_record)
                records_inserted += 1
            
            # STEP 8: Insert NEW cash account reconciliations
            for cash_acct in cash_accounts:
                cash_record = CashAccountReconciliation(
                    header_id=header.id,
                    property_id=upload.property_id,
                    period_id=upload.period_id,
                    upload_id=upload.id,
                    account_name=cash_acct.get("account_name"),
                    account_type=cash_acct.get("account_type"),
                    beginning_balance=Decimal(str(cash_acct.get("beginning_balance", 0))),
                    ending_balance=Decimal(str(cash_acct.get("ending_balance", 0))),
                    difference=Decimal(str(cash_acct.get("difference", 0))),
                    is_escrow_account=cash_acct.get("is_escrow_account", False),
                    is_negative_balance=cash_acct.get("is_negative_balance", False),
                    is_total_row=cash_acct.get("is_total_row", False),
                    line_number=cash_acct.get("line_number"),
                    page_number=cash_acct.get("page"),
                    extraction_confidence=Decimal(str(cash_acct.get("confidence", confidence_score))),
                    needs_review=cash_acct.get("confidence", confidence_score) < 85.0
                )
                self.db.add(cash_record)
                records_inserted += 1
            
            # Commit all insertions
            self.db.commit()
            logger.info(f"Successfully inserted {records_inserted} cash flow records for upload_id={upload.id}")
            return records_inserted
        
        except Exception as e:
            # Rollback insertion transaction on error
            self.db.rollback()
            error_msg = f"Failed to insert cash flow data for upload_id={upload.id}: {str(e)}"
            logger.error(error_msg, exc_info=True)
            # Re-raise with more context
            raise ValueError(f"Cash flow data insertion failed: {str(e)}") from e
    
    def _calculate_cash_flow_totals(self, items: List[Dict]) -> Dict:
        """Calculate summary totals from line items"""
        totals = {
            "total_income": 0,
            "base_rentals": 0,
            "total_operating_expenses": 0,
            "total_additional_expenses": 0,
            "total_expenses": 0,
            "noi": 0,
            "noi_percentage": 0,
            "mortgage_interest": 0,
            "depreciation": 0,
            "amortization": 0,
            "total_other": 0,
            "net_income": 0,
            "net_income_percentage": 0,
            "cash_flow": 0,
            "cash_flow_percentage": 0,
            "beginning_cash": 0,
            "ending_cash": 0
        }
        
        for item in items:
            amount = item.get("period_amount", 0)
            section = item.get("line_section", "")
            category = item.get("line_category", "")
            subcategory = item.get("line_subcategory", "")
            
            # Total Income
            if section == "INCOME" and item.get("is_total"):
                totals["total_income"] = amount
            elif section == "INCOME" and subcategory == "Base Rentals":
                totals["base_rentals"] += amount
            
            # Operating Expenses
            elif section == "OPERATING_EXPENSE" and "Total Operating" in category:
                totals["total_operating_expenses"] = amount
            
            # Additional Expenses
            elif section == "ADDITIONAL_EXPENSE" and "Total Additional" in category:
                totals["total_additional_expenses"] = amount
            
            # Total Expenses
            elif "Total Expenses" in category or (section == "OPERATING_EXPENSE" and item.get("is_total")):
                totals["total_expenses"] = amount
            
            # NOI
            elif "Net Operating Income" in subcategory or subcategory == "NOI":
                totals["noi"] = amount
            
            # Performance Metrics
            elif section == "PERFORMANCE_METRICS":
                if "Mortgage Interest" in subcategory:
                    totals["mortgage_interest"] = amount
                elif "Depreciation" in subcategory:
                    totals["depreciation"] = amount
                elif "Amortization" in subcategory:
                    totals["amortization"] = amount
            
            # Net Income
            elif "Net Income" in subcategory and "Operating" not in subcategory:
                totals["net_income"] = amount
        
        # Calculate derived values
        if totals["total_income"] > 0:
            totals["noi_percentage"] = round((totals["noi"] / totals["total_income"]) * 100, 2)
            totals["net_income_percentage"] = round((totals["net_income"] / totals["total_income"]) * 100, 2)
        
        # Cash flow will be calculated with adjustments in the calling method
        
        return totals
    
    def _parse_period_dates(self, start_str: str, end_str: str, period_id: int):
        """Parse period dates from header strings"""
        from datetime import datetime
        from app.models.financial_period import FinancialPeriod
        
        # Try to parse from strings
        if start_str and end_str:
            try:
                # Format: "Jan 2024"
                start_date = datetime.strptime(start_str, "%b %Y").date()
                end_date = datetime.strptime(end_str, "%b %Y").date()
                # Set to last day of month for end date
                if end_date.month == 12:
                    end_date = end_date.replace(day=31)
                else:
                    end_date = end_date.replace(month=end_date.month + 1, day=1)
                    end_date = end_date.replace(day=end_date.day - 1)
                return start_date, end_date
            except:
                pass
        
        # Fallback: Get from period record
        period = self.db.query(FinancialPeriod).filter(FinancialPeriod.id == period_id).first()
        if period:
            return period.period_start_date, period.period_end_date
        
        return None, None
    
    def _parse_report_date(self, date_str: str):
        """Parse report generation date"""
        from datetime import datetime
        
        if not date_str:
            return None
        
        # Try various formats
        formats = [
            "%A, %B %d, %Y",  # "Thursday, February 19, 2025"
            "%m/%d/%Y",        # "02/19/2025"
            "%Y-%m-%d"         # "2025-02-19"
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str, fmt).date()
            except:
                continue
        
        return None
    
    def _insert_rent_roll_data(
        self,
        upload: DocumentUpload,
        parsed_data: Dict,
        confidence_score: float
    ) -> int:
        """
        Insert rent roll tenant data - Template v2.0 implementation
        
        DELETE and REPLACE strategy: Deletes all existing rent roll data
        for this property+period before inserting new data to ensure clean replacement.
        
        Extracts all 24 fields, links gross rent rows, runs validation,
        and calculates quality scores.
        """
        from app.utils.rent_roll_validator import RentRollValidator
        from datetime import datetime
        
        items = parsed_data.get("line_items", [])
        report_date = parsed_data.get("report_date")
        records_inserted = 0
        parent_row_map = {}  # Maps unit_number -> parent_row_id for gross rent linking
        
        # DELETE all existing rent roll data for this property+period
        deleted_count = self.db.query(RentRollData).filter(
            RentRollData.property_id == upload.property_id,
            RentRollData.period_id == upload.period_id
        ).delete(synchronize_session=False)
        
        if deleted_count > 0:
            print(f"üóëÔ∏è  Deleted {deleted_count} existing rent roll records for property {upload.property_id}, period {upload.period_id}")
        
        # Run validation on all records
        validator = RentRollValidator()
        validation_flags = validator.validate_records(items, report_date)
        quality_result = validator.calculate_quality_score()
        
        print(f"Rent Roll Validation: Quality Score = {quality_result['quality_score']:.1f}%")
        print(f"  Critical: {quality_result['critical_count']}, Warnings: {quality_result['warning_count']}, Info: {quality_result['info_count']}")
        print(f"  Recommendation: {quality_result['recommendation']}")
        
        # Process each item
        for idx, item in enumerate(items, 1):
            unit_number = item.get("unit_number", "")
            if not unit_number:
                continue
            
            # Get validation flags for this record
            record_flags = validator.get_flags_for_record(idx)
            notes_list = [f"[{flag.severity}] {flag.message}" for flag in record_flags]
            existing_notes = item.get('notes') or ''
            all_notes = existing_notes + ("\n" if existing_notes else "") + "\n".join(notes_list) if notes_list else existing_notes
            notes = all_notes if all_notes else None
            
            # Count flags by severity
            critical_count = sum(1 for f in record_flags if f.severity == 'CRITICAL')
            warning_count = sum(1 for f in record_flags if f.severity == 'WARNING')
            info_count = sum(1 for f in record_flags if f.severity == 'INFO')
            
            # Serialize validation flags to JSON
            import json
            validation_flags_json = json.dumps([{
                'severity': f.severity,
                'field': f.field,
                'message': f.message,
                'expected': f.expected,
                'actual': f.actual
            } for f in record_flags]) if record_flags else None
            
            # Calculate individual record validation score
            record_score = 100.0 - (critical_count * 5.0) - (warning_count * 1.0)
            record_score = max(0.0, min(100.0, record_score))
            
            # Prepare all fields
            tenant_name = item.get("tenant_name") or "VACANT"
            is_vacant = item.get("is_vacant", False)
            is_gross_rent_row = item.get("is_gross_rent_row", False)
            occupancy_status = "vacant" if is_vacant else "occupied"
            
            # Determine lease status
            lease_status = item.get("lease_status", "active")
            if not lease_status and not is_gross_rent_row and not is_vacant:
                # Calculate lease status if not provided
                lease_start = item.get("lease_start_date")
                lease_end = item.get("lease_end_date")
                if report_date:
                    try:
                        report_dt = datetime.strptime(report_date, '%Y-%m-%d').date()
                        if lease_start:
                            start_dt = datetime.strptime(lease_start, '%Y-%m-%d').date()
                            if start_dt > report_dt:
                                lease_status = "future"
                        if not lease_end:
                            lease_status = "month_to_month"
                        elif lease_end:
                            end_dt = datetime.strptime(lease_end, '%Y-%m-%d').date()
                            if end_dt < report_dt:
                                lease_status = "expired"
                    except:
                        lease_status = "active"
            
            # Get parent_row_id for gross rent rows
            parent_row_id = None
            if is_gross_rent_row and unit_number in parent_row_map:
                parent_row_id = parent_row_map[unit_number]
            
            # Helper to safely convert to Decimal
            def to_decimal(val):
                if val is None:
                    return None
                try:
                    return Decimal(str(val))
                except:
                    return None
            
            # Helper to safely convert date strings
            def to_date(date_str):
                if not date_str:
                    return None
                try:
                    return datetime.strptime(date_str, '%Y-%m-%d').date()
                except:
                    return None
            
            # Create new record
            # Extract coordinates if available (for PDF source navigation)
            extraction_x0 = item.get("extraction_x0")
            extraction_y0 = item.get("extraction_y0")
            extraction_x1 = item.get("extraction_x1")
            extraction_y1 = item.get("extraction_y1")
            page_number = item.get("page_number") or item.get("page")
            
            rr_data = RentRollData(
                property_id=upload.property_id,
                period_id=upload.period_id,
                upload_id=upload.id,
                # Basic fields
                unit_number=unit_number,
                tenant_name=tenant_name,
                tenant_code=item.get("tenant_id"),
                lease_type=item.get("lease_type"),
                # Dates
                lease_start_date=to_date(item.get("lease_start_date")),
                lease_end_date=to_date(item.get("lease_end_date")),
                lease_term_months=item.get("lease_term_months"),
                # Area
                unit_area_sqft=to_decimal(item.get("unit_area_sqft")),
                # Financials
                monthly_rent=to_decimal(item.get("monthly_rent")),
                monthly_rent_per_sqft=to_decimal(item.get("monthly_rent_per_sqft")),
                annual_rent=to_decimal(item.get("annual_rent")),
                annual_rent_per_sqft=to_decimal(item.get("annual_rent_per_sqft")),
                security_deposit=to_decimal(item.get("security_deposit")),
                loc_amount=to_decimal(item.get("loc_amount")),
                # Template v2.0 fields
                tenancy_years=to_decimal(item.get("tenancy_years")),
                annual_recoveries_per_sf=to_decimal(item.get("annual_recoveries_per_sf")),
                annual_misc_per_sf=to_decimal(item.get("annual_misc_per_sf")),
                is_gross_rent_row=is_gross_rent_row,
                parent_row_id=parent_row_id,
                notes=notes,
                # Status
                occupancy_status=occupancy_status,
                lease_status=lease_status,
                # Extraction metadata
                extraction_confidence=Decimal(str(confidence_score)),
                needs_review=(critical_count > 0 or record_score < 99.0),
                # PDF Source Navigation: Extraction coordinates
                page_number=page_number,
                extraction_x0=Decimal(str(extraction_x0)) if extraction_x0 is not None else None,
                extraction_y0=Decimal(str(extraction_y0)) if extraction_y0 is not None else None,
                extraction_x1=Decimal(str(extraction_x1)) if extraction_x1 is not None else None,
                extraction_y1=Decimal(str(extraction_y1)) if extraction_y1 is not None else None
            )
            self.db.add(rr_data)
            self.db.flush()  # Get the ID for parent_row_map
            
            # Store ID for gross rent row linking
            if not is_gross_rent_row:
                parent_row_map[unit_number] = rr_data.id
            
            records_inserted += 1
        
        self.db.commit()
        
        # Log extraction summary
        print(f"Inserted {records_inserted} rent roll records")
        print(f"Quality: {quality_result['quality_score']:.1f}% - {quality_result['recommendation']}")
        
        return records_inserted
    
    def _insert_mortgage_statement_data(
        self,
        upload: DocumentUpload,
        parsed_data: Dict,
        confidence_score: float
    ) -> int:
        """
        Insert mortgage statement data
        
        DELETE and REPLACE strategy: Deletes all existing mortgage data
        for this property+period before inserting new data.
        """
        from app.services.mortgage_extraction_service import MortgageExtractionService
        from datetime import date
        
        mortgage_data = parsed_data.get("mortgage_data", {})
        if not mortgage_data:
            return 0
        
        records_inserted = 0
        
        # DELETE all existing mortgage data for this property+period
        deleted_count = self.db.query(MortgageStatementData).filter(
            MortgageStatementData.property_id == upload.property_id,
            MortgageStatementData.period_id == upload.period_id
        ).delete(synchronize_session=False)
        
        if deleted_count > 0:
            print(f"üóëÔ∏è  Deleted {deleted_count} existing mortgage records for property {upload.property_id}, period {upload.period_id}")
        
        # Helper to safely convert to Decimal
        def to_decimal(val):
            if val is None:
                return None
            if isinstance(val, Decimal):
                return val
            try:
                return Decimal(str(val))
            except:
                return None
        
        # Helper to safely convert date
        def to_date(date_val):
            if not date_val:
                return None
            if isinstance(date_val, date):
                return date_val
            if isinstance(date_val, datetime):
                return date_val.date()
            if isinstance(date_val, str):
                try:
                    return datetime.strptime(date_val, '%Y-%m-%d').date()
                except:
                    pass
            return None
        
        # Extract coordinates if available
        extraction_coords = parsed_data.get("extraction_coordinates", {})
        extraction_coordinates_json = json.dumps(extraction_coords) if extraction_coords else None
        
        # Get loan number (required)
        loan_number = mortgage_data.get("loan_number")
        if not loan_number:
            print("‚ö†Ô∏è  Warning: No loan number extracted from mortgage statement")
            loan_number = "UNKNOWN"
        
        # Create mortgage statement record
        mortgage_record = MortgageStatementData(
            property_id=upload.property_id,
            period_id=upload.period_id,
            upload_id=upload.id,
            lender_id=mortgage_data.get("lender_id"),
            
            # Loan Identification
            loan_number=str(loan_number),
            loan_type=mortgage_data.get("loan_type"),
            property_address=mortgage_data.get("property_address"),
            borrower_name=mortgage_data.get("borrower_name"),
            
            # Statement Metadata
            statement_date=to_date(mortgage_data.get("statement_date")),
            payment_due_date=to_date(mortgage_data.get("payment_due_date")),
            statement_period_start=to_date(mortgage_data.get("statement_period_start")),
            statement_period_end=to_date(mortgage_data.get("statement_period_end")),
            
            # Current Balances
            principal_balance=to_decimal(mortgage_data.get("principal_balance")) or Decimal('0'),
            tax_escrow_balance=to_decimal(mortgage_data.get("tax_escrow_balance")) or Decimal('0'),
            insurance_escrow_balance=to_decimal(mortgage_data.get("insurance_escrow_balance")) or Decimal('0'),
            reserve_balance=to_decimal(mortgage_data.get("reserve_balance")) or Decimal('0'),
            other_escrow_balance=to_decimal(mortgage_data.get("other_escrow_balance")) or Decimal('0'),
            suspense_balance=to_decimal(mortgage_data.get("suspense_balance")) or Decimal('0'),
            total_loan_balance=to_decimal(mortgage_data.get("total_loan_balance")),
            
            # Current Period Payment Breakdown
            principal_due=to_decimal(mortgage_data.get("principal_due")),
            interest_due=to_decimal(mortgage_data.get("interest_due")),
            tax_escrow_due=to_decimal(mortgage_data.get("tax_escrow_due")),
            insurance_escrow_due=to_decimal(mortgage_data.get("insurance_escrow_due")),
            reserve_due=to_decimal(mortgage_data.get("reserve_due")),
            late_fees=to_decimal(mortgage_data.get("late_fees")) or Decimal('0'),
            other_fees=to_decimal(mortgage_data.get("other_fees")) or Decimal('0'),
            total_payment_due=to_decimal(mortgage_data.get("total_payment_due")),
            
            # Year-to-Date Totals
            ytd_principal_paid=to_decimal(mortgage_data.get("ytd_principal_paid")) or Decimal('0'),
            ytd_interest_paid=to_decimal(mortgage_data.get("ytd_interest_paid")) or Decimal('0'),
            ytd_taxes_disbursed=to_decimal(mortgage_data.get("ytd_taxes_disbursed")) or Decimal('0'),
            ytd_insurance_disbursed=to_decimal(mortgage_data.get("ytd_insurance_disbursed")) or Decimal('0'),
            ytd_reserve_disbursed=to_decimal(mortgage_data.get("ytd_reserve_disbursed")) or Decimal('0'),
            ytd_total_paid=to_decimal(mortgage_data.get("ytd_total_paid")),
            
            # Loan Terms
            original_loan_amount=to_decimal(mortgage_data.get("original_loan_amount")),
            interest_rate=to_decimal(mortgage_data.get("interest_rate")),
            loan_term_months=mortgage_data.get("loan_term_months"),
            maturity_date=to_date(mortgage_data.get("maturity_date")),
            origination_date=to_date(mortgage_data.get("origination_date")),
            payment_frequency=mortgage_data.get("payment_frequency"),
            amortization_type=mortgage_data.get("amortization_type"),
            
            # Calculated Fields
            remaining_term_months=mortgage_data.get("remaining_term_months"),
            annual_debt_service=to_decimal(mortgage_data.get("annual_debt_service")),
            monthly_debt_service=to_decimal(mortgage_data.get("monthly_debt_service")),
            
            # Extraction Quality
            extraction_confidence=Decimal(str(confidence_score)),
            extraction_method=parsed_data.get("extraction_method", "template_patterns"),
            extraction_coordinates=extraction_coordinates_json,
            
            # Review Workflow
            needs_review=confidence_score < 85.0,
            reviewed=False,
            
            # Validation
            validation_score=None,  # Will be calculated by validation service
            has_errors=False
        )
        
        self.db.add(mortgage_record)
        self.db.commit()
        self.db.refresh(mortgage_record)
        
        records_inserted = 1
        
        print(f"‚úÖ Inserted mortgage statement record (loan_number: {loan_number}, confidence: {confidence_score:.1f}%)")
        
        return records_inserted
    
    def _calculate_financial_metrics(self, upload: DocumentUpload):
        """
        Calculate comprehensive financial metrics using MetricsService
        
        Calculates all 35 metrics from all available statement types:
        - Balance sheet ratios
        - Income statement margins
        - Cash flow summaries
        - Rent roll occupancy
        - Performance metrics
        
        After metrics calculation, automatically triggers alert evaluation.
        """
        try:
            from app.services.metrics_service import MetricsService
            
            metrics_service = MetricsService(self.db)
            metrics = metrics_service.calculate_all_metrics(
                property_id=upload.property_id,
                period_id=upload.period_id
            )
            
            # Trigger alert evaluation after metrics calculation (Phase 2)
            if metrics:
                try:
                    alert_trigger = AlertTriggerService(self.db)
                    alerts = alert_trigger.evaluate_and_trigger_alerts(
                        property_id=upload.property_id,
                        period_id=upload.period_id,
                        metrics=metrics
                    )
                    if alerts:
                        print(f"‚úÖ Triggered {len(alerts)} alerts for property {upload.property_id}, period {upload.period_id}")
                except Exception as alert_error:
                    # Don't fail extraction if alert generation fails
                    print(f"‚ö†Ô∏è  Alert evaluation failed (non-blocking): {str(alert_error)}")
            
            return metrics
        
        except Exception as e:
            # Log error but don't fail extraction
            print(f"Warning: Metrics calculation failed for upload {upload.id}: {str(e)}")
            return None
    
    def _detect_anomalies_for_document(self, upload: DocumentUpload):
        """
        Detect anomalies in extracted financial data for a document.
        
        Compares current period values against historical data to identify
        statistical anomalies (Z-score, percentage change).
        """
        try:
            from app.models.financial_period import FinancialPeriod
            
            # Get the period for this upload
            period = self.db.query(FinancialPeriod).filter(
                FinancialPeriod.id == upload.period_id
            ).first()
            
            if not period:
                return
            
            # Initialize anomaly detector
            detector = StatisticalAnomalyDetector(self.db)
            
            # Detect anomalies based on document type
            if upload.document_type == 'income_statement':
                self._detect_income_statement_anomalies(upload, period, detector)
            elif upload.document_type == 'balance_sheet':
                self._detect_balance_sheet_anomalies(upload, period, detector)
            elif upload.document_type == 'cash_flow':
                self._detect_cash_flow_anomalies(upload, period, detector)
            
        except Exception as e:
            # Log error but don't fail extraction
            print(f"‚ö†Ô∏è  Anomaly detection error: {str(e)}")
    
    def _detect_income_statement_anomalies(
        self, 
        upload: DocumentUpload, 
        period: 'FinancialPeriod',
        detector: StatisticalAnomalyDetector
    ):
        """Detect anomalies in income statement data"""
        from app.models.income_statement_data import IncomeStatementData
        from app.models.financial_period import FinancialPeriod
        from datetime import timedelta
        
        # Get current period data
        current_data = self.db.query(IncomeStatementData).filter(
            IncomeStatementData.property_id == upload.property_id,
            IncomeStatementData.period_id == upload.period_id
        ).all()
        
        if not current_data:
            return
        
        # Get historical periods (last 12 months)
        # Use period_start_date for comparison to avoid excluding periods that end on the same date
        cutoff_date = period.period_start_date - timedelta(days=365) if period.period_start_date else period.period_end_date - timedelta(days=365)
        historical_periods = self.db.query(FinancialPeriod).filter(
            FinancialPeriod.property_id == upload.property_id,
            FinancialPeriod.id != period.id,  # Exclude current period by ID
            FinancialPeriod.period_end_date >= cutoff_date
        ).order_by(FinancialPeriod.period_end_date.desc()).limit(12).all()
        
        if len(historical_periods) < 1:  # Lowered from 3 to 1 - need at least 1 historical period for comparison
            return
        
        # Get historical data for key accounts
        historical_period_ids = [p.id for p in historical_periods]
        historical_data = self.db.query(IncomeStatementData).filter(
            IncomeStatementData.property_id == upload.property_id,
            IncomeStatementData.period_id.in_(historical_period_ids)
        ).all()
        
        # Group by account code and period - aggregate by period first, then by account
        # This ensures we compare period totals, not individual line items
        period_account_totals = {}
        
        # First, aggregate current period by account
        current_totals = {}
        for item in current_data:
            account_code = item.account_code
            if account_code not in current_totals:
                current_totals[account_code] = 0
            current_totals[account_code] += float(item.period_amount or 0)
        
        # Then, aggregate historical periods by account
        historical_totals_by_period = {}
        for item in historical_data:
            period_id = item.period_id
            account_code = item.account_code
            if period_id not in historical_totals_by_period:
                historical_totals_by_period[period_id] = {}
            if account_code not in historical_totals_by_period[period_id]:
                historical_totals_by_period[period_id][account_code] = 0
            historical_totals_by_period[period_id][account_code] += float(item.period_amount or 0)
        
        # Build account groups with historical values aggregated by period
        account_groups = {}
        for account_code in set(list(current_totals.keys()) + [code for period_data in historical_totals_by_period.values() for code in period_data.keys()]):
            account_groups[account_code] = {
                'current': [current_totals.get(account_code, 0)],
                'historical': [historical_totals_by_period[pid].get(account_code, 0) for pid in historical_totals_by_period.keys()]
            }
        
        # Get threshold service for fetching thresholds
        from app.services.anomaly_threshold_service import AnomalyThresholdService
        threshold_service = AnomalyThresholdService(self.db)
        
        # Detect anomalies for each account
        for account_code, data in account_groups.items():
            if not data['current']:
                continue
            
            current_value = sum(data['current'])
            historical_values = data['historical']
            
            # Filter out zero values from historical
            historical_values_filtered = [v for v in historical_values if v != 0]
            if len(historical_values_filtered) < 1:  # Need at least 1 historical value for comparison
                continue
            
            # Get threshold for this account code (returns default if not set)
            threshold_value = float(threshold_service.get_threshold_value(account_code))
            
            # Run anomaly detection with filtered historical values and threshold
            result = detector.detect_anomalies(
                document_id=upload.id,
                field_name=account_code,
                current_value=current_value,
                historical_values=historical_values_filtered,
                threshold_value=threshold_value
            )
            
            # Create anomaly_detections records
            if result.get('anomalies'):
                for anomaly in result['anomalies']:
                    # Calculate confidence based on extraction accuracy
                    confidence = self._calculate_anomaly_confidence(
                        anomaly=anomaly,
                        historical_count=len(historical_values_filtered),
                        current_value=current_value,
                        expected_value=statistics.mean(historical_values_filtered),
                        upload=upload,
                        field_name=account_code
                    )
                    
                    self._create_anomaly_detection(
                        upload=upload,
                        field_name=account_code,
                        field_value=str(current_value),
                        expected_value=str(statistics.mean(historical_values)),
                        anomaly_type=anomaly.get('type', 'statistical'),
                        severity=anomaly.get('severity', 'medium'),
                        z_score=anomaly.get('z_score'),
                        percentage_change=anomaly.get('percentage_change'),
                        confidence=confidence
                    )
    
    def _detect_balance_sheet_anomalies(
        self,
        upload: DocumentUpload,
        period: 'FinancialPeriod',
        detector: StatisticalAnomalyDetector
    ):
        """Detect anomalies in balance sheet data"""
        from app.models.balance_sheet_data import BalanceSheetData
        from app.models.financial_period import FinancialPeriod
        from datetime import timedelta
        
        # Similar implementation to income statement
        current_data = self.db.query(BalanceSheetData).filter(
            BalanceSheetData.property_id == upload.property_id,
            BalanceSheetData.period_id == upload.period_id
        ).all()
        
        if not current_data:
            return
        
        cutoff_date = period.period_start_date - timedelta(days=365) if period.period_start_date else period.period_end_date - timedelta(days=365)
        historical_periods = self.db.query(FinancialPeriod).filter(
            FinancialPeriod.property_id == upload.property_id,
            FinancialPeriod.id != period.id,  # Exclude current period by ID
            FinancialPeriod.period_end_date >= cutoff_date
        ).order_by(FinancialPeriod.period_end_date.desc()).limit(12).all()
        
        if len(historical_periods) < 1:  # Lowered from 3 to 1 for balance sheet
            return
        
        historical_period_ids = [p.id for p in historical_periods]
        historical_data = self.db.query(BalanceSheetData).filter(
            BalanceSheetData.property_id == upload.property_id,
            BalanceSheetData.period_id.in_(historical_period_ids)
        ).all()
        
        # Group by account code and period - aggregate by period first
        current_totals = {}
        for item in current_data:
            account_code = item.account_code
            if account_code not in current_totals:
                current_totals[account_code] = 0
            current_totals[account_code] += float(item.amount or 0)
        
        historical_totals_by_period = {}
        for item in historical_data:
            period_id = item.period_id
            account_code = item.account_code
            if period_id not in historical_totals_by_period:
                historical_totals_by_period[period_id] = {}
            if account_code not in historical_totals_by_period[period_id]:
                historical_totals_by_period[period_id][account_code] = 0
            historical_totals_by_period[period_id][account_code] += float(item.amount or 0)
        
        account_groups = {}
        for account_code in set(list(current_totals.keys()) + [code for period_data in historical_totals_by_period.values() for code in period_data.keys()]):
            account_groups[account_code] = {
                'current': [current_totals.get(account_code, 0)],
                'historical': [historical_totals_by_period[pid].get(account_code, 0) for pid in historical_totals_by_period.keys()]
            }
        
        # Get threshold service for fetching thresholds
        from app.services.anomaly_threshold_service import AnomalyThresholdService
        threshold_service = AnomalyThresholdService(self.db)
        
        for account_code, data in account_groups.items():
            if not data['current']:
                continue
            
            current_value = sum(data['current'])
            historical_values = data['historical']
            
            # Filter out zero values but keep at least 1 non-zero value
            historical_values_filtered = [v for v in historical_values if v != 0]
            if len(historical_values_filtered) < 1:
                continue
            
            # Get threshold for this account code (returns default if not set)
            threshold_value = float(threshold_service.get_threshold_value(account_code))
            
            result = detector.detect_anomalies(
                document_id=upload.id,
                field_name=account_code,
                current_value=current_value,
                historical_values=historical_values_filtered,
                threshold_value=threshold_value
            )
            
            if result.get('anomalies'):
                for anomaly in result['anomalies']:
                    # Calculate confidence based on extraction accuracy
                    confidence = self._calculate_anomaly_confidence(
                        anomaly=anomaly,
                        historical_count=len(historical_values_filtered),
                        current_value=current_value,
                        expected_value=statistics.mean(historical_values_filtered),
                        upload=upload,
                        field_name=account_code
                    )
                    
                    self._create_anomaly_detection(
                        upload=upload,
                        field_name=account_code,
                        field_value=str(current_value),
                        expected_value=str(statistics.mean(historical_values_filtered)),
                        anomaly_type=anomaly.get('type', 'statistical'),
                        severity=anomaly.get('severity', 'medium'),
                        z_score=anomaly.get('z_score'),
                        percentage_change=anomaly.get('percentage_change'),
                        confidence=confidence
                    )
    
    def _detect_cash_flow_anomalies(
        self,
        upload: DocumentUpload,
        period: 'FinancialPeriod',
        detector: StatisticalAnomalyDetector
    ):
        """Detect anomalies in cash flow data"""
        from app.models.cash_flow_data import CashFlowData
        from app.models.financial_period import FinancialPeriod
        from datetime import timedelta
        
        # Get current period data
        current_data = self.db.query(CashFlowData).filter(
            CashFlowData.property_id == upload.property_id,
            CashFlowData.period_id == upload.period_id
        ).all()
        
        if not current_data:
            return
        
        # Get historical periods (last 12 months)
        cutoff_date = period.period_start_date - timedelta(days=365) if period.period_start_date else period.period_end_date - timedelta(days=365)
        historical_periods = self.db.query(FinancialPeriod).filter(
            FinancialPeriod.property_id == upload.property_id,
            FinancialPeriod.id != period.id,  # Exclude current period by ID
            FinancialPeriod.period_end_date >= cutoff_date
        ).order_by(FinancialPeriod.period_end_date.desc()).limit(12).all()
        
        if len(historical_periods) < 1:
            return
        
        # Get historical data for key accounts
        historical_period_ids = [p.id for p in historical_periods]
        historical_data = self.db.query(CashFlowData).filter(
            CashFlowData.property_id == upload.property_id,
            CashFlowData.period_id.in_(historical_period_ids)
        ).all()
        
        # Group by account code and period - aggregate by period first
        current_totals = {}
        for item in current_data:
            account_code = item.account_code
            if account_code not in current_totals:
                current_totals[account_code] = 0
            current_totals[account_code] += float(item.period_amount or 0)
        
        historical_totals_by_period = {}
        for item in historical_data:
            period_id = item.period_id
            account_code = item.account_code
            if period_id not in historical_totals_by_period:
                historical_totals_by_period[period_id] = {}
            if account_code not in historical_totals_by_period[period_id]:
                historical_totals_by_period[period_id][account_code] = 0
            historical_totals_by_period[period_id][account_code] += float(item.period_amount or 0)
        
        account_groups = {}
        for account_code in set(list(current_totals.keys()) + [code for period_data in historical_totals_by_period.values() for code in period_data.keys()]):
            account_groups[account_code] = {
                'current': [current_totals.get(account_code, 0)],
                'historical': [historical_totals_by_period[pid].get(account_code, 0) for pid in historical_totals_by_period.keys()]
            }
        
        # Get threshold service for fetching thresholds
        from app.services.anomaly_threshold_service import AnomalyThresholdService
        threshold_service = AnomalyThresholdService(self.db)
        
        # Detect anomalies for each account
        for account_code, data in account_groups.items():
            if not data['current']:
                continue
            
            current_value = sum(data['current'])
            historical_values = data['historical']
            
            # Filter out zero values from historical
            historical_values_filtered = [v for v in historical_values if v != 0]
            if len(historical_values_filtered) < 1:
                continue
            
            # Get threshold for this account code (returns default if not set)
            threshold_value = float(threshold_service.get_threshold_value(account_code))
            
            # Run anomaly detection with filtered historical values and threshold
            result = detector.detect_anomalies(
                document_id=upload.id,
                field_name=account_code,
                current_value=current_value,
                historical_values=historical_values_filtered,
                threshold_value=threshold_value
            )
            
            # Create anomaly_detections records
            if result.get('anomalies'):
                for anomaly in result['anomalies']:
                    # Calculate confidence based on extraction accuracy
                    confidence = self._calculate_anomaly_confidence(
                        anomaly=anomaly,
                        historical_count=len(historical_values_filtered),
                        current_value=current_value,
                        expected_value=statistics.mean(historical_values_filtered),
                        upload=upload,
                        field_name=account_code
                    )
                    
                    self._create_anomaly_detection(
                        upload=upload,
                        field_name=account_code,
                        field_value=str(current_value),
                        expected_value=str(statistics.mean(historical_values_filtered)),
                        anomaly_type=anomaly.get('type', 'statistical'),
                        severity=anomaly.get('severity', 'medium'),
                        z_score=anomaly.get('z_score'),
                        percentage_change=anomaly.get('percentage_change'),
                        confidence=confidence
                    )
    
    def _calculate_anomaly_confidence(
        self,
        anomaly: Dict,
        historical_count: int,
        current_value: float,
        expected_value: float,
        upload: DocumentUpload = None,
        field_name: str = None
    ) -> float:
        """
        Calculate confidence score based on extraction accuracy, not statistical factors.
        
        Uses the extraction_confidence from the database to determine if the value
        was correctly extracted. If extraction confidence is high (>= 95%), returns 100%.
        
        Returns:
            Confidence score between 0.0 and 1.0 (0% to 100%)
        """
        # Try to get extraction confidence from the database
        extraction_confidence = None
        
        if upload and field_name:
            try:
                # Determine document type and query appropriate table
                # Note: An account_code might have multiple line items, so we aggregate confidence
                if upload.document_type == 'income_statement':
                    from app.models.income_statement_data import IncomeStatementData
                    from sqlalchemy import func
                    # First try to get items with non-null confidence
                    items = self.db.query(IncomeStatementData).filter(
                        IncomeStatementData.upload_id == upload.id,
                        IncomeStatementData.account_code == field_name,
                        IncomeStatementData.extraction_confidence.isnot(None)
                    ).all()
                    
                    # If no items with confidence, try without the confidence filter to see if records exist
                    if not items:
                        items = self.db.query(IncomeStatementData).filter(
                            IncomeStatementData.upload_id == upload.id,
                            IncomeStatementData.account_code == field_name
                        ).all()
                    
                    if items:
                        # Use average confidence if multiple records exist (aggregated account)
                        confidences = [float(item.extraction_confidence) for item in items if item.extraction_confidence is not None]
                        if confidences:
                            # Use average confidence for aggregated accounts
                            extraction_confidence = sum(confidences) / len(confidences)
                        
                elif upload.document_type == 'balance_sheet':
                    from app.models.balance_sheet_data import BalanceSheetData
                    items = self.db.query(BalanceSheetData).filter(
                        BalanceSheetData.upload_id == upload.id,
                        BalanceSheetData.account_code == field_name,
                        BalanceSheetData.extraction_confidence.isnot(None)
                    ).all()
                    
                    if not items:
                        items = self.db.query(BalanceSheetData).filter(
                            BalanceSheetData.upload_id == upload.id,
                            BalanceSheetData.account_code == field_name
                        ).all()
                    
                    if items:
                        confidences = [float(item.extraction_confidence) for item in items if item.extraction_confidence is not None]
                        if confidences:
                            extraction_confidence = sum(confidences) / len(confidences)
                        
                elif upload.document_type == 'cash_flow':
                    from app.models.cash_flow_data import CashFlowData
                    items = self.db.query(CashFlowData).filter(
                        CashFlowData.upload_id == upload.id,
                        CashFlowData.account_code == field_name,
                        CashFlowData.extraction_confidence.isnot(None)
                    ).all()
                    
                    if not items:
                        items = self.db.query(CashFlowData).filter(
                            CashFlowData.upload_id == upload.id,
                            CashFlowData.account_code == field_name
                        ).all()
                    
                    if items:
                        confidences = [float(item.extraction_confidence) for item in items if item.extraction_confidence is not None]
                        if confidences:
                            extraction_confidence = sum(confidences) / len(confidences)
            except Exception as e:
                # If query fails, fall back to default
                import logging
                logger = logging.getLogger(__name__)
                logger.warning(f"Failed to get extraction confidence for {field_name}: {str(e)}")
                pass
        
        # If we have extraction confidence, use it directly
        if extraction_confidence is not None:
            # Convert from 0-100 scale to 0-1 scale
            confidence_decimal = extraction_confidence / 100.0
            
            # If extraction confidence is >= 95%, return 100% (1.0)
            if confidence_decimal >= 0.95:
                return 1.0
            
            # Otherwise return the actual extraction confidence
            return confidence_decimal
        
        # Fallback: if extraction confidence not available, return a default
        # You can adjust this default based on your needs
        return 0.85  # Default confidence if extraction confidence not found
    
    def _create_anomaly_detection(
        self,
        upload: DocumentUpload,
        field_name: str,
        field_value: str,
        expected_value: str,
        anomaly_type: str,
        severity: str,
        z_score: Optional[float] = None,
        percentage_change: Optional[float] = None,
        confidence: float = 0.85
    ) -> Optional[int]:
        """
        Create an anomaly_detection record with integrated intelligence.
        
        Features:
        - Active learning auto-suppression
        - Cross-property intelligence checks
        - Automatic XAI explanation generation (optional)
        
        Returns:
            Anomaly ID if created, None if suppressed
        """
        from sqlalchemy import text
        from app.models.anomaly_detection import AnomalyDetection
        
        # Map severity to database values
        severity_map = {
            'critical': 'critical',
            'high': 'high',
            'medium': 'medium',
            'low': 'low'
        }
        db_severity = severity_map.get(severity.lower(), 'medium')
        
        # Calculate actual_value for active learning check (but don't store in model - use raw SQL for that)
        try:
            actual_value = float(field_value) if field_value and field_value.replace('.', '').replace('-', '').replace('e', '').replace('E', '').replace('+', '').isdigit() else None
        except (ValueError, AttributeError):
            actual_value = None
        
        # Create anomaly detection object first (for active learning check)
        # Note: Model doesn't have property_id, account_code, or actual_value fields
        # These will be inserted via raw SQL below
        anomaly = AnomalyDetection(
            document_id=upload.id,
            field_name=field_name,
            field_value=field_value[:500] if field_value else None,
            expected_value=expected_value[:500] if expected_value else None,
            anomaly_type=anomaly_type,
            severity=db_severity,
            confidence=confidence,
            z_score=z_score,
            percentage_change=percentage_change
        )
        
        # Check for auto-suppression using active learning
        if self.anomaly_active_learning:
            should_suppress, pattern = self.anomaly_active_learning.should_suppress_anomaly(anomaly)
            if should_suppress:
                logger.info(f"Anomaly auto-suppressed by learned pattern: {pattern.id if pattern else 'unknown'}")
                return None  # Suppressed, don't create
        
        # Check cross-property intelligence
        if self.cross_property_intel and actual_value is not None:
            try:
                cross_prop_anomaly = self.cross_property_intel.detect_cross_property_anomalies(
                    property_id=upload.property_id,
                    account_code=field_name,
                    value=actual_value,
                    metric_type=upload.document_type
                )
                if cross_prop_anomaly:
                    # Enhance anomaly with cross-property information
                    import json
                    anomaly.metadata_json = json.dumps({
                        'cross_property': cross_prop_anomaly,
                        'portfolio_z_score': cross_prop_anomaly.get('z_score'),
                        'percentile_rank': cross_prop_anomaly.get('percentile_rank')
                    })
                    # Adjust severity if cross-property indicates extreme outlier
                    if cross_prop_anomaly.get('severity') == 'high' and db_severity != 'critical':
                        db_severity = 'high'
            except Exception as e:
                logger.warning(f"Cross-property intelligence check failed: {e}")
        
        # Insert anomaly detection record
        # Note: Database table doesn't have property_id, account_code, or actual_value columns
        # Store property_id and actual_value in metadata_json if needed for future reference
        metadata = anomaly.metadata_json
        if metadata is None:
            import json
            metadata = {}
        else:
            import json
            metadata = json.loads(metadata) if isinstance(metadata, str) else metadata
        
        # Add property_id and actual_value to metadata for reference
        metadata['property_id'] = upload.property_id
        metadata['account_code'] = field_name
        if actual_value is not None:
            metadata['actual_value'] = actual_value
        
        sql = text("""
            INSERT INTO anomaly_detections 
            (document_id, field_name, field_value, expected_value, 
             anomaly_type, severity, confidence, z_score, percentage_change, metadata, detected_at)
            VALUES (:document_id, :field_name, :field_value, :expected_value,
                    :anomaly_type, :severity, :confidence, :z_score, :percentage_change, :metadata_json, NOW())
            RETURNING id
        """)
        
        # Convert NumPy types to native Python types for database insertion
        def convert_numpy(val):
            if val is None:
                return None
            try:
                import numpy as np
                if isinstance(val, (np.integer, np.floating)):
                    return float(val)
                elif isinstance(val, np.ndarray):
                    return float(val.item()) if val.size == 1 else None
            except (ImportError, AttributeError, ValueError):
                pass
            return float(val) if isinstance(val, (int, float)) else val
        
        result = self.db.execute(sql, {
            'document_id': upload.id,
            'field_name': field_name,
            'field_value': field_value[:500] if field_value else None,
            'expected_value': expected_value[:500] if expected_value else None,
            'anomaly_type': anomaly_type,
            'severity': db_severity,
            'confidence': convert_numpy(confidence),
            'z_score': convert_numpy(z_score),
            'percentage_change': convert_numpy(percentage_change),
            'metadata_json': json.dumps(metadata) if metadata else None
        })
        
        anomaly_id = result.scalar()
        self.db.commit()
        
        # Auto-generate XAI explanation for high-severity anomalies (optional, can be feature-flagged)
        if self.xai_service and db_severity in ('high', 'critical') and FeatureFlags.is_shap_enabled():
            try:
                self.xai_service.generate_explanation(
                    anomaly_id=anomaly_id,
                    method='root_cause'  # Fast root cause analysis for auto-generation
                )
            except Exception as e:
                logger.warning(f"Failed to auto-generate XAI explanation: {e}")
        
        return anomaly_id
    
    def _extract_with_tables(self, pdf_data: bytes, document_type: str) -> Dict:
        """
        Extract financial data using table structure preservation
        
        Uses FinancialTableParser for highest accuracy
        Returns: structured data with proper column alignment
        """
        try:
            if document_type == "balance_sheet":
                return self.table_parser.extract_balance_sheet_table(pdf_data)
            elif document_type == "income_statement":
                return self.table_parser.extract_income_statement_table(pdf_data)
            elif document_type == "cash_flow":
                return self.table_parser.extract_cash_flow_table(pdf_data)
            elif document_type == "rent_roll":
                return self.table_parser.extract_rent_roll_table(pdf_data)
            elif document_type == "mortgage_statement":
                # Mortgage statements use template-based extraction (field patterns)
                # Table extraction not as applicable for mortgage statements
                return {
                    "success": False,
                    "error": "Use template extraction for mortgage statements"
                }
            else:
                return {
                    "success": False,
                    "error": f"Unknown document type: {document_type}"
                }
        except Exception as e:
            return {
                "success": False,
                "error": f"Table extraction failed: {str(e)}"
            }
    
    def _match_accounts_intelligent(self, extracted_items: List[Dict], document_type: str = None) -> List[Dict]:
        """
        Intelligent account matching with multiple strategies (Template v1.0 compliant)

        Strategies (in order):
        1. Exact code match
        2. Fuzzy code match (handles OCR errors - 90%+ similarity)
        3. Exact name match
        4. Fuzzy name match (85%+ similarity - Template v1.0 requirement)
        5. Partial name match with category filtering (85%+)
        6. Log unmapped for review

        Args:
            extracted_items: List of extracted line items from PDF
            document_type: Type of document being processed (for auto-account creation)

        Returns: enhanced items with matched_account_id
        """
        from fuzzywuzzy import fuzz
        
        # Get all active accounts from chart
        all_accounts = self.db.query(ChartOfAccounts).filter(
            ChartOfAccounts.is_active == True
        ).all()
        
        # Create lookup dictionaries
        accounts_by_code = {acc.account_code: acc for acc in all_accounts}
        accounts_by_name = {acc.account_name.lower(): acc for acc in all_accounts}
        
        # Group accounts by category for better matching
        accounts_by_category = {}
        for account in all_accounts:
            category = account.category or 'unknown'
            if category not in accounts_by_category:
                accounts_by_category[category] = []
            accounts_by_category[category].append(account)
        
        enhanced_items = []
        
        for item in extracted_items:
            account_code = item.get("account_code", "")
            account_name = item.get("account_name", "")
            account_category = item.get("account_category")  # From extraction
            matched_account = None
            match_method = None
            match_confidence = 0.0
            
            # Strategy 1: Exact code match
            if account_code and account_code in accounts_by_code:
                matched_account = accounts_by_code[account_code]
                match_method = "exact_code"
                match_confidence = 100.0
            
            # Strategy 2: Fuzzy code match (for OCR errors like 0 vs O, 1 vs I)
            elif account_code:
                best_match = None
                best_score = 0
                for code, account in accounts_by_code.items():
                    score = fuzz.ratio(account_code, code)
                    if score > best_score and score >= 90:  # 90%+ similarity for codes
                        best_score = score
                        best_match = account
                
                if best_match:
                    matched_account = best_match
                    match_method = "fuzzy_code"
                    match_confidence = float(best_score)
            
            # Strategy 3: Exact name match
            if not matched_account and account_name:
                name_lower = account_name.lower().strip()
                if name_lower in accounts_by_name:
                    matched_account = accounts_by_name[name_lower]
                    match_method = "exact_name"
                    match_confidence = 100.0
            
            # Strategy 4: Fuzzy name match (Template v1.0: 85%+ threshold)
            if not matched_account and account_name:
                best_match = None
                best_score = 0
                
                # Try category-specific matching first for better accuracy
                target_accounts = all_accounts
                if account_category and account_category.upper() in accounts_by_category:
                    target_accounts = accounts_by_category[account_category.upper()]
                
                for account in target_accounts:
                    # Use token_set_ratio for better matching of variations
                    score = fuzz.token_set_ratio(account_name.lower(), account.account_name.lower())
                    if score > best_score and score >= 85:  # 85%+ Template v1.0 requirement
                        best_score = score
                        best_match = account
                
                # If no match in category, try all accounts
                if not best_match and account_category:
                    for account in all_accounts:
                        score = fuzz.token_set_ratio(account_name.lower(), account.account_name.lower())
                        if score > best_score and score >= 85:
                            best_score = score
                            best_match = account
                
                if best_match:
                    matched_account = best_match
                    match_method = "fuzzy_name"
                    match_confidence = float(best_score)
            
            # Strategy 5: Partial name match with variations (for abbreviations)
            if not matched_account and account_name and len(account_name) > 5:
                best_match = None
                best_score = 0
                
                # Generate account name variations
                name_variations = [
                    account_name.replace("A/R", "Accounts Receivable"),
                    account_name.replace("A/P", "Accounts Payable"),
                    account_name.replace("Accum.", "Accumulated"),
                    account_name.replace("Depr.", "Depreciation"),
                    account_name.replace("Amort.", "Amortization"),
                    account_name.replace("&", "and")
                ]
                
                for account in all_accounts:
                    for variation in name_variations:
                        score = fuzz.token_set_ratio(variation.lower(), account.account_name.lower())
                        if score > best_score and score >= 85:
                            best_score = score
                            best_match = account
                
                if best_match:
                    matched_account = best_match
                    match_method = "fuzzy_name_variation"
                    match_confidence = float(best_score)
            
            # Enhance item with match information
            enhanced_item = item.copy()
            if matched_account:
                enhanced_item["matched_account_id"] = matched_account.id
                enhanced_item["matched_account_code"] = matched_account.account_code
                enhanced_item["matched_account_name"] = matched_account.account_name
                enhanced_item["match_method"] = match_method
                enhanced_item["match_confidence"] = match_confidence
                # Flag for review if confidence < 95% (Template v1.0 recommendation)
                enhanced_item["needs_review"] = match_confidence < 95.0
            else:
                # AUTO-CREATE missing account for intelligent system
                if account_name and account_name.strip():
                    print(f"üîß Auto-creating missing account: {account_name} (code: {account_code or 'auto-generated'})")
                    new_account = self._auto_create_account(
                        account_name=account_name,
                        account_code=account_code,
                        document_type=document_type
                    )
                    if new_account:
                        enhanced_item["matched_account_id"] = new_account.id
                        enhanced_item["matched_account_code"] = new_account.account_code
                        enhanced_item["matched_account_name"] = new_account.account_name
                        enhanced_item["match_method"] = "auto_created"
                        enhanced_item["match_confidence"] = 100.0
                        enhanced_item["needs_review"] = True  # Always review auto-created accounts
                        print(f"‚úÖ Auto-created account: {new_account.account_code} - {new_account.account_name}")
                    else:
                        # Fallback if auto-creation fails
                        enhanced_item["matched_account_id"] = None
                        enhanced_item["match_method"] = "unmapped"
                        enhanced_item["match_confidence"] = 0.0
                        enhanced_item["needs_review"] = True
                else:
                    # No account name to create from
                    enhanced_item["matched_account_id"] = None
                    enhanced_item["match_method"] = "unmapped"
                    enhanced_item["match_confidence"] = 0.0
                    enhanced_item["needs_review"] = True

            enhanced_items.append(enhanced_item)
        
        return enhanced_items
    
    def _fallback_extraction(self, text: str, document_type: str) -> Dict:
        """
        Basic fallback extraction when template matching fails
        
        Simple regex-based extraction as last resort
        """
        lines = text.split("\n")
        line_items = []
        
        # Very basic pattern matching
        for line in lines:
            # Look for patterns like: "Account Name 1,234.56" or "1234-0000 Account Name 1,234.56"
            pattern = r'([\d-]+)?\s*([A-Za-z\s&,.-]+)\s+([\d,.-]+)'
            match = re.search(pattern, line)
            
            if match:
                account_code = match.group(1) or ""
                account_name = match.group(2).strip()
                amount_str = match.group(3).replace(",", "")
                
                try:
                    amount = float(amount_str)
                    line_items.append({
                        "account_code": account_code,
                        "account_name": account_name,
                        "amount": amount,
                        "period_amount": amount
                    })
                except ValueError:
                    continue
        
        return {
            "success": len(line_items) > 0,
            "line_items": line_items,
            "confidence": 50.0  # Low confidence for fallback
        }
    
    def _run_validations(self, upload: DocumentUpload) -> Dict:
        """
        Run validation rules on extracted data
        
        Integrates ValidationService into extraction workflow
        Returns validation summary
        """
        try:
            from app.services.validation_service import ValidationService
            
            validation_service = ValidationService(self.db, tolerance_percentage=1.0)
            validation_results = validation_service.validate_upload(upload.id)
            
            # Update upload needs_review flag if validations failed
            if validation_results.get("failed_checks", 0) > 0:
                upload.notes = f"Validation failed: {validation_results['failed_checks']} checks failed"
            
            return validation_results
        
        except Exception as e:
            # Log error but don't fail extraction
            return {
                "success": False,
                "error": f"Validation error: {str(e)}",
                "total_checks": 0,
                "passed_checks": 0,
                "failed_checks": 0
            }
    
    def _capture_and_save_metadata(
        self,
        upload_id: int,
        pdf_data: bytes,
        inserted_records: Dict[str, List[Dict[str, Any]]]
    ) -> Dict[str, Any]:
        """
        Capture extraction metadata from all engines and save to database.
        
        This is the NEW SPRINT 1 feature that tracks confidence at the field level.
        
        Args:
            upload_id: Document upload ID
            pdf_data: Original PDF bytes
            inserted_records: Dict mapping table names to lists of inserted records
                Example: {'balance_sheet_data': [{'id': 1, 'account_name': 'Cash', ...}]}
        
        Returns:
            Dict with metadata capture results and statistics
        """
        try:
            print(f"üìä Capturing field-level confidence metadata...")

            # Step 1: Run all 6 engines for comprehensive field-level confidence
            # Memory limit increased to 8GB to support all engines simultaneously
            # Engines: PyMuPDF, PDFPlumber, Camelot, OCR, LayoutLM, EasyOCR
            extraction_results = self.extraction_engine.extract_with_confidence(
                pdf_data=pdf_data,
                run_all_engines=True  # Full metadata capture with 8GB memory
            )
            
            if not extraction_results or len(extraction_results) == 0:
                return {
                    "success": False,
                    "error": "No extraction results returned from engines"
                }
            
            print(f"‚úÖ Collected results from {len(extraction_results)} engines:")
            for result in extraction_results:
                status = "‚úÖ" if result.success else "‚ùå"
                print(f"   {status} {result.engine_name}: confidence {result.confidence_score:.2%}")
            
            # Step 2: Calculate aggregate confidence
            aggregation = self.confidence_engine.aggregate_extraction_results(extraction_results)
            
            print(f"üìà Aggregate confidence: {aggregation['aggregate_confidence']:.2%}")
            print(f"   Engines succeeded: {aggregation['successful_engines']}/{aggregation['total_engines']}")
            
            if aggregation.get('needs_review'):
                print(f"   ‚ö†Ô∏è  Flagged for {aggregation['review_priority']} priority review")
            
            # Step 3: Save metadata for each extracted field
            # Note: In production, this would map actual field values from each engine
            # For now, we save metadata for the inserted records
            
            result = self.metadata_service.save_extraction_result_metadata(
                document_id=upload_id,
                extraction_results=extraction_results,
                extracted_records=inserted_records
            )
            
            if not result.get("success"):
                # Metadata save failed - this will trigger rollback
                return result
            
            print(f"‚úÖ Metadata saved: {result['total_fields']} fields tracked")
            
            if result['flagged_for_review'] > 0:
                print(f"   ‚ö†Ô∏è  {result['flagged_for_review']} fields flagged for review ({result['review_percentage']:.1f}%)")
                if result['critical_fields'] > 0:
                    print(f"   üö® {result['critical_fields']} CRITICAL fields need immediate review")
            
            return {
                "success": True,
                "metadata_saved": True,
                "aggregate_confidence": float(aggregation['aggregate_confidence']),
                "total_fields_tracked": result['total_fields'],
                "flagged_for_review": result['flagged_for_review'],
                "critical_fields": result['critical_fields'],
                "engines_used": aggregation['engines_used'],
                "extraction_results": extraction_results  # Keep for further use
            }
        
        except Exception as e:
            print(f"‚ùå Metadata capture failed: {str(e)}")
            return {
                "success": False,
                "error": f"Metadata capture error: {str(e)}",
                "metadata_saved": False
            }
    
    def _auto_create_account(self, account_name: str, account_code: str = None, document_type: str = None) -> ChartOfAccounts:
        """
        Automatically create a missing account in the chart of accounts

        This makes the system intelligent and self-maintaining - new accounts are
        automatically added when encountered in PDFs.

        Args:
            account_name: Name of the account from PDF
            account_code: Account code from PDF (optional, will be auto-generated if missing)
            document_type: Type of document (helps determine account type)

        Returns:
            ChartOfAccounts object or None if creation fails
        """
        try:
            # DEFENSIVE: Validate required parameters
            if not account_name or not isinstance(account_name, str):
                print(f"‚ùå Auto-create failed: Invalid account_name parameter: {account_name}")
                return None

            if not account_name.strip():
                print(f"‚ùå Auto-create failed: Empty account_name after stripping")
                return None

            # Log detailed context for debugging
            print(f"üîß Auto-creating account with context:")
            print(f"   - Account Name: {account_name}")
            print(f"   - Account Code: {account_code or 'auto-generate'}")
            print(f"   - Document Type: {document_type or 'not specified'}")

            # Determine account type and category from name patterns
            account_type, category = self._infer_account_type_category(account_name, document_type)

            # Generate account code if not provided
            if not account_code or account_code == "UNMATCHED":
                account_code = self._generate_account_code(account_name, account_type)

            # Check if account already exists with this code (race condition protection)
            existing = self.db.query(ChartOfAccounts).filter(
                ChartOfAccounts.account_code == account_code
            ).first()

            if existing:
                print(f"   ‚ÑπÔ∏è  Account already exists with code {account_code}, using existing")
                return existing

            # Create new account
            new_account = ChartOfAccounts(
                account_code=account_code,
                account_name=account_name.strip(),
                account_type=account_type,
                category=category,
                is_active=True,
                is_calculated=False,
                display_order=9999  # Put auto-created accounts at the end
            )

            self.db.add(new_account)
            self.db.flush()  # Get the ID without committing yet

            return new_account

        except Exception as e:
            print(f"‚ùå Failed to auto-create account '{account_name}': {e}")
            return None

    def _infer_account_type_category(self, account_name: str, document_type: str = None) -> tuple:
        """
        Infer account type and category from account name patterns

        Args:
            account_name: Name of the account to analyze
            document_type: Optional document type for context-aware inference

        Returns: (account_type, category) tuple

        Raises:
            ValueError: If account_name is invalid
        """
        # DEFENSIVE: Validate input
        if not account_name or not isinstance(account_name, str):
            raise ValueError(f"Invalid account_name: {account_name}")

        name_lower = account_name.lower().strip()

        if not name_lower:
            raise ValueError("Account name is empty after stripping")

        # Asset patterns
        if any(word in name_lower for word in ['cash', 'bank', 'receivable', 'a/r', 'deposit', 'prepaid', 'asset']):
            return ('asset', 'current_asset')
        elif any(word in name_lower for word in ['land', 'building', 'equipment', 'improvement', 'property']):
            return ('asset', 'fixed_asset')
        elif any(word in name_lower for word in ['accumulated', 'depreciation', 'amortization']):
            return ('asset', 'contra_asset')

        # Liability patterns
        elif any(word in name_lower for word in ['payable', 'a/p', 'accrued', 'loan', 'mortgage', 'note', 'liability']):
            if any(word in name_lower for word in ['long', 'term', 'mortgage', 'note']):
                return ('liability', 'long_term_liability')
            return ('liability', 'current_liability')

        # Equity patterns
        elif any(word in name_lower for word in ['equity', 'capital', 'contribution', 'distribution', 'retained', 'earnings']):
            return ('equity', 'capital')

        # Income patterns (from income statement)
        elif any(word in name_lower for word in ['income', 'revenue', 'rent', 'rental', 'reimbursement', 'sales']):
            return ('income', 'rental_income')

        # Expense patterns (from income statement)
        elif any(word in name_lower for word in ['expense', 'cost', 'fee', 'tax', 'insurance', 'utility', 'maintenance', 'repair']):
            return ('expense', 'operating_expense')

        # Default based on document type
        elif document_type == 'balance_sheet':
            return ('asset', 'other_asset')  # Default to asset for balance sheet
        elif document_type == 'income_statement':
            return ('expense', 'operating_expense')  # Default to expense for income statement
        else:
            return ('asset', 'other_asset')  # Ultimate fallback

    def _generate_account_code(self, account_name: str, account_type: str) -> str:
        """
        Generate an account code for auto-created accounts

        Uses the naming convention: [type_prefix][sequential_number]
        - Assets: 0xxx-0000
        - Liabilities: 2xxx-0000
        - Equity: 3xxx-0000
        - Income: 4xxx-0000
        - Expense: 5xxx-0000

        Args:
            account_name: Name of the account (for logging)
            account_type: Type of account (asset, liability, equity, income, expense)

        Returns:
            Generated account code string

        Raises:
            ValueError: If account_type is invalid
        """
        # DEFENSIVE: Validate inputs
        if not account_type or not isinstance(account_type, str):
            raise ValueError(f"Invalid account_type: {account_type}")

        account_type = account_type.lower().strip()

        # Determine prefix based on account type
        type_prefixes = {
            'asset': '0',
            'liability': '2',
            'equity': '3',
            'income': '4',
            'expense': '5'
        }

        if account_type not in type_prefixes:
            print(f"‚ö†Ô∏è  Unknown account type '{account_type}' for '{account_name}', using fallback prefix '9'")

        prefix = type_prefixes.get(account_type, '9')  # 9xxx for unknown

        # Find the next available code in this range
        # Query for highest code in this prefix range
        highest_code = self.db.query(ChartOfAccounts.account_code).filter(
            ChartOfAccounts.account_code.like(f'{prefix}%')
        ).order_by(ChartOfAccounts.account_code.desc()).first()

        if highest_code and highest_code[0]:
            # Extract number and increment
            try:
                code_num = int(highest_code[0].split('-')[0])
                next_num = code_num + 1
            except:
                # Fallback: use 9900 series for auto-created
                next_num = int(f'{prefix}900')
        else:
            # No existing codes in this range, start at X900
            next_num = int(f'{prefix}900')

        # Format as XXXX-0000
        return f'{next_num:04d}-0000'

    def _determine_cash_flow_category(self, account_code: str, line_section: str) -> str:
        """
        Determine cash flow category (operating/investing/financing) based on account code
        
        Mapping:
        - 4000-4999: Income ‚Üí Operating
        - 5000-5999: Operating Expenses ‚Üí Operating
        - 6000-6999: Additional Expenses ‚Üí Operating
        - 7000-7999: Other (Interest, Depreciation) ‚Üí Financing/Investing
        - 1000-1999: Assets ‚Üí Investing
        - 2000-2999: Liabilities ‚Üí Financing
        - 3000-3999: Equity ‚Üí Financing
        """
        if not account_code or account_code == "UNMATCHED":
            # Fallback to line_section
            if line_section in ['INCOME', 'OPERATING_EXPENSE', 'ADDITIONAL_EXPENSE']:
                return 'operating'
            elif line_section == 'ADJUSTMENTS':
                return 'financing'
            return None
        
        # Parse first digit of account code
        try:
            first_digit = int(account_code[0])
            
            # Revenue (4xxx) ‚Üí Operating
            if first_digit == 4:
                return 'operating'
            
            # Expenses (5xxx, 6xxx) ‚Üí Operating
            elif first_digit in [5, 6]:
                return 'operating'
            
            # Other expenses (7xxx) - Interest/Depreciation
            elif first_digit == 7:
                if 'interest' in account_code.lower() or '7010' in account_code:
                    return 'financing'
                else:
                    return 'operating'  # Depreciation is non-cash, but part of operating
            
            # Assets (1xxx) ‚Üí Investing
            elif first_digit == 1:
                return 'investing'
            
            # Liabilities & Equity (2xxx, 3xxx) ‚Üí Financing
            elif first_digit in [2, 3]:
                return 'financing'
            
            else:
                return 'operating'  # Default
                
        except (ValueError, IndexError):
            return 'operating'  # Default fallback

